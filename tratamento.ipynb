{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', None)\n",
    "data=pd.read_csv('train.csv') # dataset do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dtypes\n",
    "data.info() #milage\n",
    "# data.head()\n",
    "# data.tail()\n",
    "# data.shape #(3207, 12)\n",
    "# data.nunique() # valores unicos para cada coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo mais antigo e mais novo:\n",
    "antigo=data['model_year'].min()\n",
    "novo=data['model_year'].max()\n",
    "print(antigo,novo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correção: 'milage' -> INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#milage:\n",
    "valores_corrigidos=[]\n",
    "for milage in data['milage']: # para cada quilometragem do dataset\n",
    "    separa=re.split(r'[ ]',milage) # sepagar o numero do mi\n",
    "    numero=separa[0] # numero\n",
    "    letra=separa[1] # mi\n",
    "    verifica=re.search(r'^[0-9]+[,]?[0-9]+$',numero) # verifica se o numeros sao sempre iguais(com casas decimais ou não)\n",
    "    verifica2=re.search(r'^mi\\.$',letra) # verifica se a letra é sempre mi\n",
    "    if verifica and verifica2: # se seguir o padra numero + mi:\n",
    "        numero=int(re.sub(r',','',numero)) # retira a , dos numeros e passa para inteiro ( estavam em obj)\n",
    "        valores_corrigidos.append(numero)\n",
    "    else: \n",
    "        print(milage,False) # tem dados diferentes no dataset\n",
    "\n",
    "data['milage']=valores_corrigidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificaçao\n",
    "for m in data['milage']:\n",
    "    if not isinstance(m, (int)):\n",
    "        print('dado incorreto')\n",
    "# tudo certo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NULL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuel_type'].value_counts() # –  38 ( existem 38 linhas com (-) -> nao se sabe)\n",
    "data['accident'].value_counts()\n",
    "data['clean_title'].value_counts() # Yes -> 2740, os restantes são valores nulos (nan)\n",
    "# data['clean_title'].unique()\n",
    "# verificar outros tipos de dizer valores nulos: ( como (-) por exemplo)\n",
    "data['brand'].value_counts() # tudo certo\n",
    "data['model'].nunique() # tudo certo\n",
    "data['model_year'].value_counts() # tudo certo\n",
    "data['engine'].value_counts() # – 38 -> nao se sabe\n",
    "data['transmission'].value_counts() # 4 -> nao se sabe \n",
    "# data['ext_col'].value_counts()# 11 -> nao se sabe\n",
    "# data['int_col'].value_counts() # 98 -> nao se sabe\n",
    "# for preco in data['price']: # tudo certo\n",
    "#     if not isinstance(preco, int):\n",
    "#         print('erro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['milage'])\n",
    "# len(data['milage'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --> nº de velocidades na transmissão existentes no dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2=list(data['transmission'].unique())\n",
    "alls = [int(numero) for string in d2 for numero in re.findall(r'\\d+', string)]\n",
    "list(set(alls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verificar significado de '-' e NaN para o tipo de combustível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.loc[data['fuel_type'] == '–'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['fuel_type'] == '–') & (data['engine'] != '–')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OS CARROS PARA OS QUAIS O FUEL_TYPE É '-' NÃO INDICADO, TAMBÉM NÃO SE CONHECE A CONFIGURAÇÃO DO MOTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAR QUE OS CARROS COM NAN NO TIPO DE COMBUSTÍVEL SÃO CARROS ELÉTRICOS\n",
    "elec=data[data['fuel_type'].isnull()]\n",
    "pattern = re.compile(r'\\bElectric\\b', flags=re.IGNORECASE)\n",
    "contains_electric = elec['engine'].str.contains(pattern, na=False)\n",
    "# Selecionar todas as linhas que não contêm 'Electric' na coluna 'engine'\n",
    "elec[~contains_electric]\n",
    "# elec[contains_electric].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### os carros com NaN para 'fuel_type' são carros elétricos\n",
    "Tesla --> elétrico\n",
    "Standard Range Battery --> elétrico\n",
    "111.2Ah / FR 70kW / RR 160kW (697V) --> especificidade de baterias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOXPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='box',figsize=(15,6),subplots=True) # grafico do codigo acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['model_year']<1990] # outlier do 1º gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['milage']>350000] # outlier do 2º gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['price']>1500000] # outlier 3º gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BARPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Supondo que 'data' seja seu DataFrame com colunas de atributos e 'preco'\n",
    "# Vamos calcular a média do preço para cada atributo (exceto a última coluna)\n",
    "data['fuel_type'].fillna('Elétrico', inplace=True)\n",
    "\n",
    "# Lista para armazenar os gráficos gerados\n",
    "colunas = ['brand', 'model', 'model_year', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
    "num_linhas = 3\n",
    "num_colunas = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_linhas, num_colunas, figsize=(15, 10))\n",
    "\n",
    "# Iterar sobre as colunas do DataFrame\n",
    "for i, column in enumerate(colunas[:-1]):\n",
    "    # Calcular a média do preço para cada valor único na coluna\n",
    "    med = data.groupby(column)['price'].mean()\n",
    "    top = med.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    # Truncate long labels and append ellipsis\n",
    "    truncated_labels = [str(val)[:10] + '...' if len(str(val)) > 10 else str(val) for val in top.index]\n",
    "\n",
    "    # Determine the subplot index\n",
    "    linha = i // num_colunas\n",
    "    coluna = i % num_colunas\n",
    "\n",
    "    # Plotar o gráfico de barras para a média do preço por valor\n",
    "    axs[linha, coluna].bar(truncated_labels, top.values, color='deepskyblue')\n",
    "    axs[linha, coluna].set_title(f'Média de Preço por {column.upper()}')\n",
    "    axs[linha, coluna].set_ylabel('Média de Preço')\n",
    "    axs[linha, coluna].tick_params(axis='x', rotation=45)  # Rotacionar rótulos do eixo x\n",
    "\n",
    "# Ajustar o layout para evitar sobreposição\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### análise dos modelos com preço mais altos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data.sort_values(by='price', ascending=False).head(10)\n",
    "d[['brand', 'model', 'price']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LINEPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.groupby('milage')['price'].mean()\n",
    "d=d.sort_index()\n",
    "plt.scatter(d.index, d.values, marker='o', linestyle='-')\n",
    "plt.title('Preço Médio em Função do Número de Quilômetros')\n",
    "plt.xlabel('Quilometragem')\n",
    "plt.ylabel('Preço Médio')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()  # Ajuste automático da disposição para evitar sobreposição\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### o preço diminui a medida que o nº de quilometros aumenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = data.groupby(['brand', 'model_year'])['price'].describe()\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ENGINE] novos atributos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Potencia'] = data['engine'].str.extract(r'(\\d+\\.\\d+)HP?')\n",
    "data['Capacidade_Motor'] = data['engine'].str.extract(r'(\\d+\\.\\d+|\\d+)\\s*(?:L|Liter)')\n",
    "data['Numero_Cilindros'] = data['engine'].str.extract(r'(?:V(\\d+)|I-(\\d+)|I(\\d+)|(\\d+) Cylinder)').apply(lambda x: next(filter(lambda y: pd.notna(y), x), None), axis=1)\n",
    "# data['Tipo_Combustivel'] = data['engine'].str.extract(r'(Gasoline Fuel|Flexible Fuel|Electric)')\n",
    "data['Numero_Valvulas'] = data['engine'].str.extract(r'( \\d+)V')\n",
    "\n",
    "data['Potencia'] = pd.to_numeric(data['Potencia'], errors='coerce')\n",
    "data['Capacidade_Motor'] = pd.to_numeric(data['Capacidade_Motor'], errors='coerce')\n",
    "data['Numero_Cilindros'] = pd.to_numeric(data['Numero_Cilindros'], errors='coerce')\n",
    "data['Numero_Valvulas'] = pd.to_numeric(data['Numero_Valvulas'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=len(data[data['Numero_Valvulas'].isnull()])\n",
    "d\n",
    "# d[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(50)\n",
    "# data[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BARPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame with columns of attributes and 'price'\n",
    "# Let's calculate the mean price for each attribute (except the last column)\n",
    "\n",
    "# List to store the generated plots\n",
    "columns = ['Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas']\n",
    "\n",
    "num_linhas = 2\n",
    "num_colunas = 2\n",
    "\n",
    "fig, axs = plt.subplots(num_linhas, num_colunas, figsize=(10, 5))\n",
    "\n",
    "# Iterar sobre as colunas do DataFrame\n",
    "for i, column in enumerate(columns):\n",
    "    # Calcular a média do preço para cada valor único na coluna\n",
    "    med = data.groupby(column)['price'].mean()\n",
    "    top = med.sort_values(ascending=False).head(13)\n",
    "    \n",
    "    # Truncate long labels and append ellipsis\n",
    "    truncated_labels = [str(val)[:10] + '...' if len(str(val)) > 10 else str(val) for val in top.index]\n",
    "\n",
    "    # Determine the subplot index\n",
    "    linha = i // num_colunas\n",
    "    coluna = i % num_colunas\n",
    "\n",
    "    # Plotar o gráfico de barras para a média do preço por valor\n",
    "    axs[linha, coluna].bar(truncated_labels, top.values, color='blueviolet')\n",
    "    axs[linha, coluna].set_title(f'Média de Preço por {column.upper()}')\n",
    "    axs[linha, coluna].set_ylabel('Média de Preço')\n",
    "    axs[linha, coluna].tick_params(axis='x', rotation=45)  # Rotacionar rótulos do eixo x\n",
    "\n",
    "# Ajustar o layout para evitar sobreposição\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[data['Numero_Valvulas'] == 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Numero_Valvulas'].unique())\n",
    "sorted(list(data['Numero_Valvulas'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Numero_Cilindros'].unique())\n",
    "list(sorted(data['Numero_Cilindros'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data['Capacidade_Motor'].unique())\n",
    "#sorted(list(data['Capacidade_Motor'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(data['Potencia'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verificação de possíveis valores únicos reportados pela análise do boxplot preço / marca no R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['brand'] == 'Maybach'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('train_ccols.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SUBSETS] com dataset W/ ENGINE caract-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', None)\n",
    "p=pd.read_csv('train_ccols.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituir pela mediana :\n",
    "- num valvulas\n",
    "- num cilindros\n",
    "- potencia\n",
    "- capacidade_motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = p['Numero_Valvulas'].median()\n",
    "m2 = p['Numero_Cilindros'].median()\n",
    "m3 = p['Potencia'].median()\n",
    "m4 = p['Capacidade_Motor'].median()\n",
    "\n",
    "p['Numero_Valvulas'].fillna(m, inplace=True)\n",
    "p['Numero_Cilindros'].fillna(m2, inplace=True)\n",
    "p['Potencia'].fillna(m3, inplace=True)\n",
    "p['Capacidade_Motor'].fillna(m4, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['clean_title'] = p['clean_title'].fillna('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### novos atributos (derivações)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "p['T2'] = p['transmission'].str.extract(r'(\\d+)')\n",
    "p['T2'] = pd.to_numeric(p['T2'], errors='coerce')\n",
    "m5 = p['T2'].mean() \n",
    "p['T2'].fillna(round(m5), inplace=True)\n",
    "#p.head()\n",
    "#p['T2'].isnull().sum()\n",
    "# p['T2'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria_ET (engine + transmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = p[['Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "n_clusters = 5\n",
    "\n",
    "# Aplicar o algoritmo K-means para agrupar os carros em clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Adicionar uma nova coluna 'Categoria' ao DataFrame com base nos clusters\n",
    "p['Categoria_ET'] = kmeans.labels_\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Contagem de amostras em cada categoria\n",
    "# categoria_counts = p['Categoria_ET'].value_counts().sort_index()\n",
    "\n",
    "# # Criar o gráfico de barras\n",
    "# plt.bar(categoria_counts.index, categoria_counts.values)\n",
    "\n",
    "# # Adicionar rótulos e título\n",
    "# plt.xlabel('Categoria_ET')\n",
    "# plt.ylabel('Número de amostras')\n",
    "# plt.title('Distribuição das Categorias')\n",
    "\n",
    "# Mostrar o gráfico\n",
    "# plt.show()\n",
    "\n",
    "# p['Categoria_ET'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Categoria_ET'] = p['Categoria_ET'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria Marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = p['brand'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência Marca'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência Marca'\n",
    "    else:\n",
    "        return 'Média Frequência Marca'\n",
    "\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "p['Categoria_Marca'] = p['brand'].map(frequencias.apply(categorizar))\n",
    "\n",
    "p['Categoria_Marca'] = p['Categoria_Marca'].astype('category')\n",
    "p['Categoria_Marca'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = p['model'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência Modelo'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência Modelo'\n",
    "    else:\n",
    "        return 'Média Frequência Modelo'\n",
    "\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "p['Categoria_Modelo'] = p['model'].map(frequencias.apply(categorizar))\n",
    "p['Categoria_Modelo'] = p['Categoria_Modelo'].astype('category')\n",
    "p['Categoria_Modelo'].value_counts()\n",
    "p['Categoria_Modelo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalidade das novas colunas (numéricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Lista das colunas numéricas que você deseja analisar\n",
    "colunas_numericas = ['Potencia', 'Capacidade_Motor', 'Numero_Cilindros',\n",
    "       'Numero_Valvulas', 'T2']\n",
    "\n",
    "num_colunas = len(colunas_numericas)\n",
    "fig, axs = plt.subplots(2, num_colunas, figsize=(15, 8))\n",
    "print(f'Teste de Shapiro-Wilk: \\n')\n",
    "\n",
    "# Iterar sobre as colunas numéricas e plotar os gráficos em cada subplot\n",
    "for i, coluna in enumerate(colunas_numericas):\n",
    "    # Histograma\n",
    "    sns.histplot(p[coluna], kde=True, ax=axs[0, i])\n",
    "    axs[0, i].set_title(f'{coluna}')\n",
    "    \n",
    "    # Gráfico QQ\n",
    "    stats.probplot(p[coluna], dist=\"norm\", plot=axs[1, i])\n",
    "    axs[1, i].set_title(f'QQ-plot')\n",
    " \n",
    "    # Teste de Shapiro-Wilk\n",
    "    stat, pv = stats.shapiro(p[coluna])\n",
    "    print(f'{coluna}:')\n",
    "    print(f'Valor p: {pv}')\n",
    "    if pv > 0.05:\n",
    "        print('Não podemos rejeitar a hipótese nula - A distribuição parece normal.')\n",
    "    else:\n",
    "        print('Rejeita-se a hipótese nula -> a distribuição não segue o modelo normal.')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como se pode observar, nenhum dos novos atributos (numéricos possuem distribuição normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Substituição de valores nulos e '-' PELA MODA\n",
    "- Remoção outliers LOF\n",
    "- Variavies cat -> numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL's substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['fuel_type'].unique()\n",
    "data1['fuel_type'].isnull().sum()\n",
    "data1['fuel_type'].fillna('Eletric', inplace=True)\n",
    "mod = data1['fuel_type'].mode()[0]\n",
    "\n",
    "data1['fuel_type'] = data1['fuel_type'].replace('–', mod)\n",
    "data1['fuel_type'].unique()\n",
    "\n",
    "mod1=data1['accident'].mode()[0]\n",
    "data1['accident'].fillna(mod1, inplace=True)\n",
    "\n",
    "mod3=data1['engine'].mode()[0]\n",
    "data1['engine'] = data1['engine'].replace('–', mod3)\n",
    "\n",
    "mod5=data1['ext_col'].mode()[0]\n",
    "data1['ext_col'] = data1['ext_col'].replace('–', mod5)\n",
    "\n",
    "mod6=data1['int_col'].mode()[0]\n",
    "data1['int_col'] = data1['int_col'].replace('–', mod6)\n",
    "\n",
    "mod7=data1['transmission'].mode()[0]\n",
    "data1['transmission'] = data1['transmission'].replace('–', mod6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data1['int_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    elif limite_baixa < frequencia < limite_alta:  # Verifica se a frequência está entre os limites\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data1['Categoria_IntCol'] = data1['int_col'].map(frequencias.apply(categorizar))\n",
    "data1['Categoria_IntCol'] = data1['Categoria_IntCol'].astype('category')\n",
    "data1['Categoria_IntCol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['ext_col'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categoria cor externa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data1['ext_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    elif limite_baixa < frequencia < limite_alta:  # Verifica se a frequência está entre os limites\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data1['Categoria_ExtCol'] = data1['ext_col'].map(frequencias.apply(categorizar))\n",
    "data1['Categoria_ExtCol'] = data1['Categoria_ExtCol'].astype('category')\n",
    "data1['Categoria_ExtCol'].value_counts()\n",
    "data1['Categoria_ExtCol'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Selecionar apenas as variáveis numéricas\n",
    "numeric_df = data1.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calcular a correlação de Spearman\n",
    "spearman_corr = numeric_df.corr(method='spearman')\n",
    "\n",
    "# Visualizar a matriz de correlação de Spearman\n",
    "print(\"Matriz de correlação de Spearman:\")\n",
    "print(spearman_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#va's categóricas\n",
    "categorical_columns = data1.select_dtypes(include=['object','category']).columns\n",
    "correlations = {}\n",
    "for column in categorical_columns:\n",
    "    corr, _ = spearmanr(data1[column], data1['price'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier por IQR -> V.a Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict = {}\n",
    "\n",
    "# Variáveis Numéricas\n",
    "for column in data1.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    Q1 = data1[column].quantile(0.25)\n",
    "    Q3 = data1[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outlier_conditional = ((data1[column] > Q1 - 1.5*IQR) & (data1[column] < Q3 + 1.5*IQR))\n",
    "\n",
    "    outlier_indices_dict[column] = data1.loc[~outlier_conditional].index\n",
    "    \n",
    "    num_outliers = len(data1[~outlier_conditional])\n",
    "    print(f\"Número de outliers em '{column}': {num_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### O número de válvulas têm muito pouca correlação com o preço -> N utiliza-se essa coluna para os cenários!\n",
    "###### Remove-se os 6 registros da capacidade do motor e os 57 com num de cilindros e 376 do T2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover registros com outliers (cujo correlação baixa com preço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict['Capacidade_Motor']\n",
    "outlier_indices_dict['Numero_Cilindros']\n",
    "outlier_indices_dict['T2']\n",
    "\n",
    "data1_s_out = data1.copy()\n",
    "\n",
    "outlier_indices_to_drop = outlier_indices_dict['Capacidade_Motor'].union(outlier_indices_dict['Numero_Cilindros']).union(outlier_indices_dict['T2'])\n",
    "data1_s_out.drop(outlier_indices_to_drop, inplace=True)\n",
    "\n",
    "# Verifica o tamanho do novo conjunto de dados\n",
    "print(\"Número de registros no novo conjunto de dados sem outliers:\", len(data1_s_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('subset_1.csv', index=False) # sem normalizar com outliers\n",
    "data1_s_out.to_csv('subset_1_s_out.csv', index=False) # sem normalizar sem outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  substituir '-' por desconhecido\n",
    "- Remoção outliers LOF\n",
    "- Variavies cat -> numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL's substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuel Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['fuel_type'].unique()\n",
    "data2['fuel_type'].isnull().sum()\n",
    "data2['fuel_type'].fillna('Eletric', inplace=True)\n",
    "data2['fuel_type'] = data2['fuel_type'].replace('–', 'desconhecido')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accident + engine + transmission + ex_col + int_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['accident'].fillna('desconhecido', inplace=True)\n",
    "data2['engine'] = data2['engine'].replace('–', 'desconhecido')\n",
    "data2['transmission'] = data2['transmission'].replace('–', 'desconhecido')\n",
    "data2['ext_col'] = data2['ext_col'].replace('–', 'desconhecido')\n",
    "data2['int_col'] = data2['int_col'].replace('–', 'desconhecido')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data2['int_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência int_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência int_col'\n",
    "    else:\n",
    "        return 'Média Frequência int_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data2['Categoria_IntCol'] = data2['int_col'].map(frequencias.apply(categorizar))\n",
    "data2['Categoria_IntCol'] = data2['Categoria_IntCol'].astype('category')\n",
    "data2['Categoria_IntCol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor externa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data2['ext_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    else:\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data2['Categoria_ExtCol'] = data2['ext_col'].map(frequencias.apply(categorizar))\n",
    "data2['Categoria_ExtCol'] = data2['Categoria_ExtCol'].astype('category')\n",
    "data2['Categoria_ExtCol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Selecionar apenas as variáveis numéricas\n",
    "numeric_df = data2.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calcular a correlação de Spearman\n",
    "spearman_corr = numeric_df.corr(method='spearman')\n",
    "\n",
    "# Visualizar a matriz de correlação de Spearman\n",
    "print(\"Matriz de correlação de Spearman:\")\n",
    "print(spearman_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#va categorica\n",
    "categorical_columns = data2.select_dtypes(include=['object','category']).columns\n",
    "correlations = {}\n",
    "for column in categorical_columns:\n",
    "    corr, _ = spearmanr(data2[column], data2['price'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier por IQR -> va.'s Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis Numéricas\n",
    "for column in data2.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    Q1 = data2[column].quantile(0.25)\n",
    "    Q3 = data2[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outlier_conditional = ((data2[column] > Q1 - 1.5*IQR) & (data2[column] < Q3 + 1.5*IQR))\n",
    "    outlier_indices_dict[column] = data2.loc[~outlier_conditional].index\n",
    "    \n",
    "    num_outliers = len(data2[~outlier_conditional])\n",
    "    print(f\"Número de outliers em '{column}': {num_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover registros com outliers (cuja correlação baixa com preço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict['Capacidade_Motor']\n",
    "outlier_indices_dict['Numero_Cilindros']\n",
    "outlier_indices_dict['T2']\n",
    "\n",
    "data2_s_out = data2.copy()\n",
    "\n",
    "outlier_indices_to_drop = outlier_indices_dict['Capacidade_Motor'].union(outlier_indices_dict['Numero_Cilindros']).union(outlier_indices_dict['T2'])\n",
    "data2_s_out.drop(outlier_indices_to_drop, inplace=True)\n",
    "\n",
    "# Verifica o tamanho do novo conjunto de dados\n",
    "print(\"Número de registros no novo conjunto de dados sem outliers:\", len(data2_s_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('subset_2.csv', index=False) # sem normalizar com outliers\n",
    "data2_s_out.to_csv('subset_2_s_out.csv', index=False) # sem normalizar sem outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELAÇÃO\n",
    "### Como subset 1 é muito parecido com subset 2, só vamos utilizar o subset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3207, 22)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "subset_1=pd.read_csv('subset_1.csv')\n",
    "subset_1_s_out=pd.read_csv('subset_1_s_out.csv')\n",
    "pd.set_option('display.max_rows', None)\n",
    "# subset_1.head(1)\n",
    "subset_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 1 -> todas as variáveis (sem categorias extras (redundância))\n",
    "#### - (se for usar todas as variáveis não da para normalizar os dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = subset_1[['brand','model','model_year','milage','fuel_type','engine','ext_col','int_col','accident','clean_title']]\n",
    "y1 = subset_1['price'] \n",
    "# definir amostra treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "X1_train = pd.get_dummies(X, drop_first=True) # para var. categóricas \n",
    "X1_test = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X1_train: (2565, 10)\n",
      "Shape of y1_train: (2565,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X1_train:\", X1_train.shape)\n",
    "print(\"Shape of y1_train:\", y1_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label logaritmizada (preço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# features_num=X1.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "# features_cat=X1.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "# preprocessor = make_column_transformer(\n",
    "#     (StandardScaler(), features_num),\n",
    "#     (OneHotEncoder(), features_cat),\n",
    "# )\n",
    "\n",
    "# X1_train_norm = preprocessor.fit_transform(X1_train)\n",
    "# X1_test_norm = preprocessor.transform(X1_test)\n",
    "y1_train_log = np.log(y1_train)\n",
    "y1_test_log = np.log(y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sem outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1out = subset_1_s_out[['brand','model','model_year','milage','fuel_type','engine','ext_col','int_col','accident','clean_title']] # tinha T2 » retirei...\n",
    "y1out = subset_1_s_out['price'] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1out_train, X1out_test, y1out_train, y1out_test = train_test_split(X1out, y1out, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalizados e label logaritmizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# features_num=data2.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "# features_cat=data2.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "# preprocessor = make_column_transformer(\n",
    "#     (StandardScaler(), features_num),\n",
    "#     (OneHotEncoder(), features_cat),\n",
    "# )\n",
    "\n",
    "# X1out_train_norm = preprocessor.fit_transform(X1_train)\n",
    "# X1out_valid_norm = preprocessor.transform(X1_test)\n",
    "y1out_train_log = np.log(y1out_train) # »»» mudei y1_train para y1out_train «««\n",
    "y1out_test_log = np.log(y1out_test) # »»» y1_train para y1out_test «««"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenário 2 (todas as variáveis (categorias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = subset_1[['model_year', 'milage', 'fuel_type', 'accident', 'clean_title', 'Categoria_ET', 'Categoria_Marca', 'Categoria_Modelo', 'Categoria_IntCol', 'Categoria_ExtCol']]\n",
    "y2 = subset_1['price'] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalização + label logaritmizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X2.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X2.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "X2_train_norm = preprocessor.fit_transform(X2_train)\n",
    "X2_test_norm = preprocessor.transform(X2_test)\n",
    "y2_train_log = np.log(y2_train)\n",
    "y2_test_log = np.log(y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sem outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2out = subset_1_s_out[['model_year','milage','fuel_type','accident','clean_title','Categoria_ET','Categoria_Marca','Categoria_Modelo','Categoria_IntCol','Categoria_ExtCol']]  \n",
    "y2out = subset_1_s_out['price'] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X2out_train, X2out_test, y2out_train, y2out_test = train_test_split(X2out, y2out, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X2.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X2.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "X2out_train_norm = preprocessor.fit_transform(X2_train)\n",
    "X2out_valid_norm = preprocessor.transform(X2_test)\n",
    "y2out_train_log = np.log(y2_train)\n",
    "y2out_test_log = np.log(y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 3 (ao olho, características do carro novo sem influência do condutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[['model_year', 'Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 4 (ao olho, características do carro com influência do condutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[['model_year', 'milage' 'Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2', 'accident', 'clean_title', 'Categoria_IntCol', 'Categoria_ExtCol']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 5 = cenário 4 subsituindo 'Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2' por 'Categoria_ET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Passo 1: Instanciar o Objeto SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Passo 2: Aplicar o SMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Passo 3: Verificar o Resultado\n",
    "print(\"Tamanho do conjunto de dados antes do SMOTE:\", len(X), len(y))\n",
    "print(\"Tamanho do conjunto de dados após o SMOTE:\", len(X_resampled), len(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar Data / Treinar e Avaliar Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "# from sklearn import datasets\n",
    "from slickml.metrics import (\n",
    "    RegressionMetrics,\n",
    ")  # downloaded from https://github.com/slickml/slick-ml # btw pip install slickml\n",
    "from matplotlib import pyplot as plt\n",
    "# import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3207, 2565]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m X1_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(X1, drop_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# para var. categóricas \u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:578\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    574\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    576\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 578\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32mc:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1281\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\elisa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3207, 2565]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "X1_train = pd.get_dummies(X1, drop_first=True) # para var. categóricas \n",
    "X1_test = pd.get_dummies(X1, drop_first=True) # para var. categóricas \n",
    "model = LinearRegression()\n",
    "model.fit(X1_train, y1_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X1_train: (3207, 3148)\n",
      "Shape of y1_train: (2565,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X1_train:\", X1_train.shape)\n",
    "print(\"Shape of y1_train:\", y1_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting actual vs predicted prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
    "plt.title('Actual vs Predicted Car Prices (Multiple Linear Regression)')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict with Random Forest Regressor\n",
    "RF = RandomForestRegressor()\n",
    "RF.fit(X_train, y_train)\n",
    "pred = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics for Random Forest\n",
    "from slickml.metrics import RegressionMetrics\n",
    "\n",
    "reg_metrics = RegressionMetrics(y_test, pred)\n",
    "reg_metrics.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecionar as colunas necessárias a partir do df SEM OUTLIERS\n",
    "ck3=ck.loc[:, ['model_year', 'price', 'Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2', 'fuel_type', 'accident', 'clean_title']]\n",
    "X = ck3.drop(['price'], axis=1) \n",
    "y = ck3['price'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, drop_first=True) # para var. categóricas \n",
    "# Option 1: Fill missing values, for example with the median or mean\n",
    "X.fillna(X.median(), inplace=True)\n",
    "\n",
    "# Option 2: Drop rows with missing values\n",
    "# X.dropna(inplace=True)´\n",
    "\n",
    "# nao precisa mais desse codigo em cima dessa linha\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting actual vs predicted prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
    "plt.title('Actual vs Predicted Car Prices (Multiple Linear Regression)')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "erro um pouco menor do que R1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AD -> Subset 2  -cenário 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2 = pd.read_csv('subset_2.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir o dataset em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn import datasets\n",
    "# from slickml.metrics import (\n",
    "#     RegressionMetrics,\n",
    "# )   # downloaded from https://github.com/slickml/slick-ml\n",
    "from matplotlib import pyplot as plt\n",
    "#import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = subset2.drop(['price'], axis=1)  # sem os outliers \n",
    "y = subset2['price'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divida os dados transformados em conjuntos de treinamento e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_tr[['price', 'model_year', 'Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2','milage']] = scaler.fit_transform(X_tr[['price', 'model_year', 'Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2', 'milage']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = tree.DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "DT.fit(X_tr, y_tr)\n",
    "pred3 = DT.predict(X_ts)\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# dt1_model = DecisionTreeRegressor(random_state=0,\n",
    "#                                            criterion='mse')\n",
    "# dt1_model.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(DT, filled=True, feature_names=X_tr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics for Decision Tree\n",
    "reg_metrics3 = RegressionMetrics(y_ts, pred3)\n",
    "reg_metrics3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# Calcular as predições\n",
    "preds = DT.predict(X_ts)\n",
    "\n",
    "# Calcular métricas de regressão\n",
    "mse = mean_squared_error(y_ts, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_ts, preds)\n",
    "mae = mean_absolute_error(y_ts, preds)\n",
    "medae = median_absolute_error(y_ts, preds)\n",
    "\n",
    "# Exibir as métricas\n",
    "print(\"Erro Quadrático Médio (MSE):\", mse)\n",
    "print(\"Raiz do Erro Quadrático Médio (RMSE):\", rmse)\n",
    "print(\"Coeficiente de Determinação (R²):\", r2)\n",
    "print(\"Erro Absoluto Médio (MAE):\", mae)\n",
    "print(\"Mediana do Erro Absoluto (MedAE):\", medae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphviz\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "tree_graph = tree.export_graphviz(DT, out_file=None)\n",
    "graphviz.Source(tree_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AD -> Subset 1- Cenário1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1 = pd.read_csv('subset_1.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "subset1[['price', 'model_year', 'Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2','milage']] = scaler.fit_transform(subset1[['price', 'model_year', 'Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2', 'milage']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "X = subset1.drop(['price'], axis=1)  # sem os outliers \n",
    "y =subset1['price'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divida os dados transformados em conjuntos de treinamento e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = tree.DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "DT.fit(X_tr, y_tr)\n",
    "pred3 = DT.predict(X_ts)\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(DT, filled=True, feature_names=X_tr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
    "\n",
    "# Calcular as predições\n",
    "preds = DT.predict(X_ts)\n",
    "\n",
    "# Calcular métricas de regressão\n",
    "mse = mean_squared_error(y_ts, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_ts, preds)\n",
    "mae = mean_absolute_error(y_ts, preds)\n",
    "medae = median_absolute_error(y_ts, preds)\n",
    "\n",
    "# Exibir as métricas\n",
    "print(\"Erro Quadrático Médio (MSE):\", mse)\n",
    "print(\"Raiz do Erro Quadrático Médio (RMSE):\", rmse)\n",
    "print(\"Coeficiente de Determinação (R²):\", r2)\n",
    "print(\"Erro Absoluto Médio (MAE):\", mae)\n",
    "print(\"Mediana do Erro Absoluto (MedAE):\", medae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
