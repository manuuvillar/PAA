{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', None)\n",
    "data=pd.read_csv('train.csv') # dataset do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dtypes\n",
    "data.info() #milage\n",
    "# data.head()\n",
    "# data.tail()\n",
    "# data.shape #(3207, 12)\n",
    "# data.nunique() # valores unicos para cada coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo mais antigo e mais novo:\n",
    "antigo=data['model_year'].min()\n",
    "novo=data['model_year'].max()\n",
    "print(antigo,novo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correção: 'milage' -> INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#milage:\n",
    "valores_corrigidos=[]\n",
    "for milage in data['milage']: # para cada quilometragem do dataset\n",
    "    separa=re.split(r'[ ]',milage) # sepagar o numero do mi\n",
    "    numero=separa[0] # numero\n",
    "    letra=separa[1] # mi\n",
    "    verifica=re.search(r'^[0-9]+[,]?[0-9]+$',numero) # verifica se o numeros sao sempre iguais(com casas decimais ou não)\n",
    "    verifica2=re.search(r'^mi\\.$',letra) # verifica se a letra é sempre mi\n",
    "    if verifica and verifica2: # se seguir o padra numero + mi:\n",
    "        numero=int(re.sub(r',','',numero)) # retira a , dos numeros e passa para inteiro ( estavam em obj)\n",
    "        valores_corrigidos.append(numero)\n",
    "    else: \n",
    "        print(milage,False) # tem dados diferentes no dataset\n",
    "\n",
    "data['milage']=valores_corrigidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificaçao\n",
    "for m in data['milage']:\n",
    "    if not isinstance(m, (int)):\n",
    "        print('dado incorreto')\n",
    "# tudo certo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NULL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuel_type'].value_counts() # –  38 ( existem 38 linhas com (-) -> nao se sabe)\n",
    "data['accident'].value_counts()\n",
    "data['clean_title'].value_counts() # Yes -> 2740, os restantes são valores nulos (nan)\n",
    "# data['clean_title'].unique()\n",
    "# verificar outros tipos de dizer valores nulos: ( como (-) por exemplo)\n",
    "data['brand'].value_counts() # tudo certo\n",
    "data['model'].nunique() # tudo certo\n",
    "data['model_year'].value_counts() # tudo certo\n",
    "data['engine'].value_counts() # – 38 -> nao se sabe\n",
    "data['transmission'].value_counts() # 4 -> nao se sabe \n",
    "# data['ext_col'].value_counts()# 11 -> nao se sabe\n",
    "# data['int_col'].value_counts() # 98 -> nao se sabe\n",
    "# for preco in data['price']: # tudo certo\n",
    "#     if not isinstance(preco, int):\n",
    "#         print('erro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['milage'])\n",
    "# len(data['milage'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --> nº de velocidades na transmissão existentes no dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2=list(data['transmission'].unique())\n",
    "alls = [int(numero) for string in d2 for numero in re.findall(r'\\d+', string)]\n",
    "list(set(alls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verificar significado de '-' e NaN para o tipo de combustível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.loc[data['fuel_type'] == '–'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['fuel_type'] == '–') & (data['engine'] != '–')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OS CARROS PARA OS QUAIS O FUEL_TYPE É '-' NÃO INDICADO, TAMBÉM NÃO SE CONHECE A CONFIGURAÇÃO DO MOTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAR QUE OS CARROS COM NAN NO TIPO DE COMBUSTÍVEL SÃO CARROS ELÉTRICOS\n",
    "elec=data[data['fuel_type'].isnull()]\n",
    "pattern = re.compile(r'\\bElectric\\b', flags=re.IGNORECASE)\n",
    "contains_electric = elec['engine'].str.contains(pattern, na=False)\n",
    "# Selecionar todas as linhas que não contêm 'Electric' na coluna 'engine'\n",
    "elec[~contains_electric]\n",
    "# elec[contains_electric].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### os carros com NaN para 'fuel_type' são carros elétricos\n",
    "Tesla --> elétrico\n",
    "Standard Range Battery --> elétrico\n",
    "111.2Ah / FR 70kW / RR 160kW (697V) --> especificidade de baterias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOXPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='box',figsize=(15,6),subplots=True) # grafico do codigo acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['model_year']<1990] # outlier do 1º gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['milage']>350000] # outlier do 2º gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['price']>1500000] # outlier 3º gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BARPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Supondo que 'data' seja seu DataFrame com colunas de atributos e 'preco'\n",
    "# Vamos calcular a média do preço para cada atributo (exceto a última coluna)\n",
    "data['fuel_type'].fillna('Elétrico', inplace=True)\n",
    "\n",
    "# Lista para armazenar os gráficos gerados\n",
    "colunas = ['brand', 'model', 'model_year', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
    "num_linhas = 3\n",
    "num_colunas = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_linhas, num_colunas, figsize=(15, 10))\n",
    "\n",
    "# Iterar sobre as colunas do DataFrame\n",
    "for i, column in enumerate(colunas[:-1]):\n",
    "    # Calcular a média do preço para cada valor único na coluna\n",
    "    med = data.groupby(column)['price'].mean()\n",
    "    top = med.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    # Truncate long labels and append ellipsis\n",
    "    truncated_labels = [str(val)[:10] + '...' if len(str(val)) > 10 else str(val) for val in top.index]\n",
    "\n",
    "    # Determine the subplot index\n",
    "    linha = i // num_colunas\n",
    "    coluna = i % num_colunas\n",
    "\n",
    "    # Plotar o gráfico de barras para a média do preço por valor\n",
    "    axs[linha, coluna].bar(truncated_labels, top.values, color='deepskyblue')\n",
    "    axs[linha, coluna].set_title(f'Média de Preço por {column.upper()}')\n",
    "    axs[linha, coluna].set_ylabel('Média de Preço')\n",
    "    axs[linha, coluna].tick_params(axis='x', rotation=45)  # Rotacionar rótulos do eixo x\n",
    "\n",
    "# Ajustar o layout para evitar sobreposição\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### análise dos modelos com preço mais altos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data.sort_values(by='price', ascending=False).head(10)\n",
    "d[['brand', 'model', 'price']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LINEPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.groupby('milage')['price'].mean()\n",
    "d=d.sort_index()\n",
    "plt.scatter(d.index, d.values, marker='o', linestyle='-')\n",
    "plt.title('Preço Médio em Função do Número de Quilômetros')\n",
    "plt.xlabel('Quilometragem')\n",
    "plt.ylabel('Preço Médio')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()  # Ajuste automático da disposição para evitar sobreposição\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### o preço diminui a medida que o nº de quilometros aumenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = data.groupby(['brand', 'model_year'])['price'].describe()\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ENGINE] novos atributos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Potencia'] = data['engine'].str.extract(r'(\\d+\\.\\d+)HP?')\n",
    "data['Capacidade_Motor'] = data['engine'].str.extract(r'(\\d+\\.\\d+|\\d+)\\s*(?:L|Liter)')\n",
    "data['Numero_Cilindros'] = data['engine'].str.extract(r'(?:V(\\d+)|I-(\\d+)|I(\\d+)|(\\d+) Cylinder)').apply(lambda x: next(filter(lambda y: pd.notna(y), x), None), axis=1)\n",
    "# data['Tipo_Combustivel'] = data['engine'].str.extract(r'(Gasoline Fuel|Flexible Fuel|Electric)')\n",
    "data['Numero_Valvulas'] = data['engine'].str.extract(r'( \\d+)V')\n",
    "\n",
    "data['Potencia'] = pd.to_numeric(data['Potencia'], errors='coerce')\n",
    "data['Capacidade_Motor'] = pd.to_numeric(data['Capacidade_Motor'], errors='coerce')\n",
    "data['Numero_Cilindros'] = pd.to_numeric(data['Numero_Cilindros'], errors='coerce')\n",
    "data['Numero_Valvulas'] = pd.to_numeric(data['Numero_Valvulas'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=len(data[data['Numero_Valvulas'].isnull()])\n",
    "d\n",
    "# d[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(50)\n",
    "# data[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BARPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame with columns of attributes and 'price'\n",
    "# Let's calculate the mean price for each attribute (except the last column)\n",
    "\n",
    "# List to store the generated plots\n",
    "columns = ['Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas']\n",
    "\n",
    "num_linhas = 2\n",
    "num_colunas = 2\n",
    "\n",
    "fig, axs = plt.subplots(num_linhas, num_colunas, figsize=(10, 5))\n",
    "\n",
    "# Iterar sobre as colunas do DataFrame\n",
    "for i, column in enumerate(columns):\n",
    "    # Calcular a média do preço para cada valor único na coluna\n",
    "    med = data.groupby(column)['price'].mean()\n",
    "    top = med.sort_values(ascending=False).head(13)\n",
    "    \n",
    "    # Truncate long labels and append ellipsis\n",
    "    truncated_labels = [str(val)[:10] + '...' if len(str(val)) > 10 else str(val) for val in top.index]\n",
    "\n",
    "    # Determine the subplot index\n",
    "    linha = i // num_colunas\n",
    "    coluna = i % num_colunas\n",
    "\n",
    "    # Plotar o gráfico de barras para a média do preço por valor\n",
    "    axs[linha, coluna].bar(truncated_labels, top.values, color='blueviolet')\n",
    "    axs[linha, coluna].set_title(f'Média de Preço por {column.upper()}')\n",
    "    axs[linha, coluna].set_ylabel('Média de Preço')\n",
    "    axs[linha, coluna].tick_params(axis='x', rotation=45)  # Rotacionar rótulos do eixo x\n",
    "\n",
    "# Ajustar o layout para evitar sobreposição\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[data['Numero_Valvulas'] == 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Numero_Valvulas'].unique())\n",
    "sorted(list(data['Numero_Valvulas'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Numero_Cilindros'].unique())\n",
    "list(sorted(data['Numero_Cilindros'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data['Capacidade_Motor'].unique())\n",
    "#sorted(list(data['Capacidade_Motor'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(data['Potencia'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verificação de possíveis valores únicos reportados pela análise do boxplot preço / marca no R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['brand'] == 'Maybach'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('train_ccols.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SUBSETS] com dataset W/ ENGINE caract-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', None)\n",
    "p=pd.read_csv('train_ccols.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituir pela mediana :\n",
    "- num valvulas\n",
    "- num cilindros\n",
    "- potencia\n",
    "- capacidade_motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = p['Numero_Valvulas'].median()\n",
    "m2 = p['Numero_Cilindros'].median()\n",
    "m3 = p['Potencia'].median()\n",
    "m4 = p['Capacidade_Motor'].median()\n",
    "\n",
    "p['Numero_Valvulas'].fillna(m, inplace=True)\n",
    "p['Numero_Cilindros'].fillna(m2, inplace=True)\n",
    "p['Potencia'].fillna(m3, inplace=True)\n",
    "p['Capacidade_Motor'].fillna(m4, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['clean_title'] = p['clean_title'].fillna('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### novos atributos (derivações)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "p['T2'] = p['transmission'].str.extract(r'(\\d+)')\n",
    "p['T2'] = pd.to_numeric(p['T2'], errors='coerce')\n",
    "m5 = p['T2'].mean() \n",
    "p['T2'].fillna(round(m5), inplace=True)\n",
    "#p.head()\n",
    "#p['T2'].isnull().sum()\n",
    "# p['T2'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria ET (engine + transmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = p[['Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "n_clusters = 5\n",
    "\n",
    "# Aplicar o algoritmo K-means para agrupar os carros em clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Adicionar uma nova coluna 'Categoria' ao DataFrame com base nos clusters\n",
    "p['Categoria_ET'] = kmeans.labels_\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contagem de amostras em cada categoria\n",
    "categoria_counts = p['Categoria_ET'].value_counts().sort_index()\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "plt.bar(categoria_counts.index, categoria_counts.values)\n",
    "\n",
    "# Adicionar rótulos e título\n",
    "plt.xlabel('Categoria_ET')\n",
    "plt.ylabel('Número de amostras')\n",
    "plt.title('Distribuição das Categorias')\n",
    "\n",
    "#Mostrar o gráfico\n",
    "plt.show()\n",
    "\n",
    "p['Categoria_ET'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Categoria_ET'] = p['Categoria_ET'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categoria Marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = p['brand'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência Marca'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência Marca'\n",
    "    else:\n",
    "        return 'Média Frequência Marca'\n",
    "\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "p['Categoria_Marca'] = p['brand'].map(frequencias.apply(categorizar))\n",
    "\n",
    "p['Categoria_Marca'] = p['Categoria_Marca'].astype('category')\n",
    "p['Categoria_Marca'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = p['model'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência Modelo'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência Modelo'\n",
    "    else:\n",
    "        return 'Média Frequência Modelo'\n",
    "\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "p['Categoria_Modelo'] = p['model'].map(frequencias.apply(categorizar))\n",
    "p['Categoria_Modelo'] = p['Categoria_Modelo'].astype('category')\n",
    "p['Categoria_Modelo'].value_counts()\n",
    "p['Categoria_Modelo'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p=subset_1_s_out \n",
    "#### atributos auxiliares ao cenário extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=subset_1_s_out # pegar no dataset sem outliers considerado sempre como mais eficaz pelos modelos durante a modelação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categoria MarcaModelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Definir as colunas categóricas e numéricas\n",
    "v_catg = ['brand', 'model']\n",
    "v_num = ['Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2']\n",
    "\n",
    "# Criar transformador para colunas categóricas\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "# Criar um pipeline de pré-processamento que inclui OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, v_catg),\n",
    "        ('num', StandardScaler(), v_num)\n",
    "    ])\n",
    "\n",
    "# Aplicar o pré-processamento aos dados\n",
    "X = p[v_catg + v_num]\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Definir o número de clusters\n",
    "n_clusters = 13\n",
    "\n",
    "# Aplicar o algoritmo K-means para agrupar os carros em clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X_preprocessed)\n",
    "\n",
    "# Adicionar uma nova coluna 'Categoria_MarcaModelo' ao DataFrame com base nos clusters\n",
    "p['Categoria_MarcaModelo'] = kmeans.labels_\n",
    "\n",
    "# Contagem de amostras em cada categoria\n",
    "categoria_counts = p['Categoria_MarcaModelo'].value_counts().sort_index()\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "plt.bar(categoria_counts.index, categoria_counts.values)\n",
    "\n",
    "# Adicionar rótulos e título\n",
    "plt.xlabel('Categoria_MarcaModelo')\n",
    "plt.ylabel('Número de amostras')\n",
    "plt.title('Distribuição das Categorias')\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()\n",
    "\n",
    "# Mostrar categorias únicas\n",
    "print(p['Categoria_MarcaModelo'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Categoria_MarcaModelo'] = p['Categoria_MarcaModelo'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def condicao_do_veiculo(row):\n",
    "#     if row['accident'] == 'At least 1 accident or damage reported' and row['clean_title'] == 'No':\n",
    "#         return 'mau'\n",
    "#     elif row['accident'] == 'None reported' and row['clean_title'] == 'Yes':\n",
    "#         return 'bom'\n",
    "#     else:\n",
    "#         return 'avg'\n",
    "\n",
    "# p['condicao_do_veiculo'] = p.apply(condicao_do_veiculo, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularidade_do_modelo = p['model'].value_counts().to_dict()\n",
    "# p['model_popularity'] = p['model'].map(popularidade_do_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p['milage_por_ano'] = p['milage'] / (2024 - p['model_year'] + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## potencia / capacidade relação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Potencia_Capacidade_Relacao'] = p['Potencia'] / p['Capacidade_Motor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalidade das novas colunas (numéricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Lista das colunas numéricas que você deseja analisar\n",
    "colunas_numericas = ['Potencia', 'Capacidade_Motor', 'Numero_Cilindros',\n",
    "       'Numero_Valvulas', 'T2']\n",
    "\n",
    "num_colunas = len(colunas_numericas)\n",
    "fig, axs = plt.subplots(2, num_colunas, figsize=(15, 8))\n",
    "print(f'Teste de Shapiro-Wilk: \\n')\n",
    "\n",
    "# Iterar sobre as colunas numéricas e plotar os gráficos em cada subplot\n",
    "for i, coluna in enumerate(colunas_numericas):\n",
    "    # Histograma\n",
    "    sns.histplot(p[coluna], kde=True, ax=axs[0, i])\n",
    "    axs[0, i].set_title(f'{coluna}')\n",
    "    \n",
    "    # Gráfico QQ\n",
    "    stats.probplot(p[coluna], dist=\"norm\", plot=axs[1, i])\n",
    "    axs[1, i].set_title(f'QQ-plot')\n",
    " \n",
    "    # Teste de Shapiro-Wilk\n",
    "    stat, pv = stats.shapiro(p[coluna])\n",
    "    print(f'{coluna}:')\n",
    "    print(f'Valor p: {pv}')\n",
    "    if pv > 0.05:\n",
    "        print('Não podemos rejeitar a hipótese nula - A distribuição parece normal.')\n",
    "    else:\n",
    "        print('Rejeita-se a hipótese nula -> a distribuição não segue o modelo normal.')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como se pode observar, nenhum dos novos atributos (numéricos possuem distribuição normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Substituição de valores nulos e '-' PELA MODA\n",
    "- Remoção outliers LOF\n",
    "- Variavies cat -> numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL's substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['fuel_type'].unique()\n",
    "data1['fuel_type'].isnull().sum()\n",
    "data1['fuel_type'].fillna('Eletric', inplace=True)\n",
    "mod = data1['fuel_type'].mode()[0]\n",
    "\n",
    "data1['fuel_type'] = data1['fuel_type'].replace('–', mod)\n",
    "data1['fuel_type'].unique()\n",
    "\n",
    "mod1=data1['accident'].mode()[0]\n",
    "data1['accident'].fillna(mod1, inplace=True)\n",
    "\n",
    "mod3=data1['engine'].mode()[0]\n",
    "data1['engine'] = data1['engine'].replace('–', mod3)\n",
    "\n",
    "mod5=data1['ext_col'].mode()[0]\n",
    "data1['ext_col'] = data1['ext_col'].replace('–', mod5)\n",
    "\n",
    "mod6=data1['int_col'].mode()[0]\n",
    "data1['int_col'] = data1['int_col'].replace('–', mod6)\n",
    "\n",
    "mod7=data1['transmission'].mode()[0]\n",
    "data1['transmission'] = data1['transmission'].replace('–', mod6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data1['int_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    elif limite_baixa < frequencia < limite_alta:  # Verifica se a frequência está entre os limites\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data1['Categoria_IntCol'] = data1['int_col'].map(frequencias.apply(categorizar))\n",
    "data1['Categoria_IntCol'] = data1['Categoria_IntCol'].astype('category')\n",
    "data1['Categoria_IntCol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['ext_col'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categoria cor externa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data1['ext_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    elif limite_baixa < frequencia < limite_alta:  # Verifica se a frequência está entre os limites\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data1['Categoria_ExtCol'] = data1['ext_col'].map(frequencias.apply(categorizar))\n",
    "data1['Categoria_ExtCol'] = data1['Categoria_ExtCol'].astype('category')\n",
    "data1['Categoria_ExtCol'].value_counts()\n",
    "data1['Categoria_ExtCol'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Selecionar apenas as variáveis numéricas\n",
    "numeric_df = data1.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calcular a correlação de Spearman\n",
    "spearman_corr = numeric_df.corr(method='spearman')\n",
    "\n",
    "# Visualizar a matriz de correlação de Spearman\n",
    "print(\"Matriz de correlação de Spearman:\")\n",
    "print(spearman_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#va's categóricas\n",
    "categorical_columns = data1.select_dtypes(include=['object','category']).columns\n",
    "correlations = {}\n",
    "for column in categorical_columns:\n",
    "    corr, _ = spearmanr(data1[column], data1['price'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier por IQR -> va. Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict = {}\n",
    "\n",
    "# Variáveis Numéricas\n",
    "for column in data1.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    Q1 = data1[column].quantile(0.25)\n",
    "    Q3 = data1[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outlier_conditional = ((data1[column] > Q1 - 1.5*IQR) & (data1[column] < Q3 + 1.5*IQR))\n",
    "\n",
    "    outlier_indices_dict[column] = data1.loc[~outlier_conditional].index\n",
    "    \n",
    "    num_outliers = len(data1[~outlier_conditional])\n",
    "    print(f\"Número de outliers em '{column}': {num_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### O número de válvulas têm muito pouca correlação com o preço -> N utiliza-se essa coluna para os cenários!\n",
    "###### Remove-se os 6 registros da capacidade do motor e os 57 com num de cilindros e 376 do T2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover registros com outliers (cujo correlação baixa com preço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict['Capacidade_Motor']\n",
    "outlier_indices_dict['Numero_Cilindros']\n",
    "outlier_indices_dict['T2']\n",
    "\n",
    "data1_s_out = data1.copy()\n",
    "\n",
    "outlier_indices_to_drop = outlier_indices_dict['Capacidade_Motor'].union(outlier_indices_dict['Numero_Cilindros']).union(outlier_indices_dict['T2'])\n",
    "data1_s_out.drop(outlier_indices_to_drop, inplace=True)\n",
    "\n",
    "# Verifica o tamanho do novo conjunto de dados\n",
    "print(\"Número de registros no novo conjunto de dados sem outliers:\", len(data1_s_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('subset_1.csv', index=False) # sem normalizar com outliers\n",
    "data1_s_out.to_csv('subset_1_s_out.csv', index=False) # sem normalizar sem outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  substituir '-' por desconhecido\n",
    "- Remoção outliers LOF\n",
    "- Variavies cat -> numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL's substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuel Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['fuel_type'].unique()\n",
    "data2['fuel_type'].isnull().sum()\n",
    "data2['fuel_type'].fillna('Eletric', inplace=True)\n",
    "data2['fuel_type'] = data2['fuel_type'].replace('–', 'desconhecido')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accident + engine + transmission + ex_col + int_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['accident'].fillna('desconhecido', inplace=True)\n",
    "data2['engine'] = data2['engine'].replace('–', 'desconhecido')\n",
    "data2['transmission'] = data2['transmission'].replace('–', 'desconhecido')\n",
    "data2['ext_col'] = data2['ext_col'].replace('–', 'desconhecido')\n",
    "data2['int_col'] = data2['int_col'].replace('–', 'desconhecido')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data2['int_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência int_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência int_col'\n",
    "    else:\n",
    "        return 'Média Frequência int_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data2['Categoria_IntCol'] = data2['int_col'].map(frequencias.apply(categorizar))\n",
    "data2['Categoria_IntCol'] = data2['Categoria_IntCol'].astype('category')\n",
    "data2['Categoria_IntCol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor externa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data2['ext_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    else:\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data2['Categoria_ExtCol'] = data2['ext_col'].map(frequencias.apply(categorizar))\n",
    "data2['Categoria_ExtCol'] = data2['Categoria_ExtCol'].astype('category')\n",
    "data2['Categoria_ExtCol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Selecionar apenas as variáveis numéricas\n",
    "numeric_df = data2.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calcular a correlação de Spearman\n",
    "spearman_corr = numeric_df.corr(method='spearman')\n",
    "\n",
    "# Visualizar a matriz de correlação de Spearman\n",
    "print(\"Matriz de correlação de Spearman:\")\n",
    "print(spearman_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#va categorica\n",
    "categorical_columns = data2.select_dtypes(include=['object','category']).columns\n",
    "correlations = {}\n",
    "for column in categorical_columns:\n",
    "    corr, _ = spearmanr(data2[column], data2['price'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier por IQR -> va.'s Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis Numéricas\n",
    "for column in data2.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    Q1 = data2[column].quantile(0.25)\n",
    "    Q3 = data2[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outlier_conditional = ((data2[column] > Q1 - 1.5*IQR) & (data2[column] < Q3 + 1.5*IQR))\n",
    "    outlier_indices_dict[column] = data2.loc[~outlier_conditional].index\n",
    "    \n",
    "    num_outliers = len(data2[~outlier_conditional])\n",
    "    print(f\"Número de outliers em '{column}': {num_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover registros com outliers (cuja correlação baixa com preço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict['Capacidade_Motor']\n",
    "outlier_indices_dict['Numero_Cilindros']\n",
    "outlier_indices_dict['T2']\n",
    "\n",
    "data2_s_out = data2.copy()\n",
    "\n",
    "outlier_indices_to_drop = outlier_indices_dict['Capacidade_Motor'].union(outlier_indices_dict['Numero_Cilindros']).union(outlier_indices_dict['T2'])\n",
    "data2_s_out.drop(outlier_indices_to_drop, inplace=True)\n",
    "\n",
    "# Verifica o tamanho do novo conjunto de dados\n",
    "print(\"Número de registros no novo conjunto de dados sem outliers:\", len(data2_s_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('subset_2.csv', index=False) # sem normalizar com outliers\n",
    "data2_s_out.to_csv('subset_2_s_out.csv', index=False) # sem normalizar sem outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELAÇÃO\n",
    "### Como subset 1 é muito parecido com subset 2, só vamos utilizar o subset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "subset_1=pd.read_csv('subset_1.csv')\n",
    "subset_1_s_out=pd.read_csv('subset_1_s_out.csv')\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenário 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = subset_1[['brand','model','model_year','milage','fuel_type','engine','transmission','ext_col','int_col','accident','clean_title']]\n",
    "y1 = subset_1['price'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label logaritmizada (preço) + normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sem outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1out = subset_1_s_out[['brand','model','model_year','milage','fuel_type','engine','ext_col','int_col','accident','clean_title']] # tinha T2 » retirei...\n",
    "y1out = subset_1_s_out['price'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenário 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = subset_1[['model_year', 'milage', 'fuel_type', 'accident', 'clean_title', 'Categoria_ET', 'Categoria_Marca', 'Categoria_Modelo', 'Categoria_IntCol', 'Categoria_ExtCol']]\n",
    "y2 = subset_1['price'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sem outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2out = subset_1_s_out[['model_year','milage','fuel_type','accident','clean_title','Categoria_ET','Categoria_Marca','Categoria_Modelo','Categoria_IntCol','Categoria_ExtCol']]  \n",
    "y2out = subset_1_s_out['price'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenário 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3=subset_1[['Potencia','Capacidade_Motor','Numero_Cilindros','T2', 'model_year', 'milage', 'Numero_Valvulas']]\n",
    "y3=subset_1['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sem outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3out = subset_1_s_out[['Potencia','Capacidade_Motor','Numero_Cilindros','T2','model_year', 'milage', 'Numero_Valvulas']] \n",
    "y3out = subset_1_s_out['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenário 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4=subset_1[['brand','model','model_year','milage','fuel_type','ext_col','int_col','accident','clean_title']]\n",
    "y4=subset_1['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sem outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4out = subset_1_s_out[['brand','model','model_year','milage','fuel_type','ext_col','int_col','accident','clean_title']]\n",
    "y4out = subset_1_s_out['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenário Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX=p[['Potencia','Capacidade_Motor','Numero_Cilindros','T2','model_year','Numero_Valvulas', 'brand','model','milage','fuel_type','ext_col','int_col','accident','clean_title','Potencia_Capacidade_Relacao', 'Categoria_MarcaModelo']]\n",
    "y=p['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# previsões para os cenários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar Data / Treinar e Avaliar Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "# from sklearn import datasets\n",
    "from slickml.metrics import (\n",
    "    RegressionMetrics,\n",
    ")  # downloaded from https://github.com/slickml/slick-ml # btw pip install slickml\n",
    "from matplotlib import pyplot as plt\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    ")\n",
    "\n",
    "# Definir o número de folds para a cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Função para calcular Mean Absolute Percentage Error\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Função para calcular REC AUC\n",
    "def rec_auc(y_true, y_pred):\n",
    "    # Calcula o erro normalizado\n",
    "    error = np.abs(y_true - y_pred) / np.abs(y_true)\n",
    "    # Ordena os erros\n",
    "    sorted_error = np.sort(error)\n",
    "    # Frequência cumulativa relativa\n",
    "    cumulative_freq = np.arange(1, len(sorted_error) + 1) / len(sorted_error)\n",
    "    # Calcula a AUC utilizando a regra do trapezoide\n",
    "    return np.trapz(cumulative_freq, sorted_error)\n",
    "\n",
    "def calculate_rec_curve(y_true, y_pred):\n",
    "    # Calcula o erro normalizado\n",
    "    error = np.abs(y_true - y_pred) / np.abs(y_true)\n",
    "    # Ordena os erros\n",
    "    sorted_error = np.sort(error)\n",
    "    # Frequência cumulativa relativa\n",
    "    cumulative_freq = np.arange(1, len(sorted_error) + 1) / len(sorted_error)\n",
    "    return sorted_error, cumulative_freq\n",
    "\n",
    "# Função para calcular Coefficient of Variation\n",
    "def coefficient_of_variation(y_true, y_pred):\n",
    "    return np.std(y_pred) / np.mean(y_pred)\n",
    "\n",
    "# Função para calcular Mean of Variation\n",
    "def mean_of_variation(y_true, y_pred):\n",
    "    return np.mean(y_pred) / np.mean(y_true)\n",
    "\n",
    "def plot_rec_curves(curves):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for model_id, (sorted_error, cumulative_freq) in curves.items():\n",
    "        if model_id == 1:\n",
    "            label = 'Regressão Linear'\n",
    "        elif model_id == 2:\n",
    "            label = 'Random Forest'\n",
    "        elif model_id ==3:\n",
    "            label='Ada Boost'\n",
    "        elif model_id==4:\n",
    "            label='Decision Tree'\n",
    "        else: label='Neural Net'\n",
    "\n",
    "        plt.plot(sorted_error, cumulative_freq, label=label)\n",
    "        \n",
    "    \n",
    "    plt.xlabel('Relative Error')\n",
    "    plt.ylabel('Cumulative Frequency')\n",
    "    plt.title('Curva REC')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def modelo (n,X,y):\n",
    "    # Listas para armazenar os resultados da validação cruzada\n",
    "    cv_r2_scores = []\n",
    "    cv_ev_scores = []\n",
    "    cv_mae_scores = []\n",
    "    cv_mse_scores = []\n",
    "    cv_msle_scores = []\n",
    "    cv_mape_scores = []\n",
    "    cv_rec_auc_scores = []\n",
    "    cv_coeff_var_scores = []\n",
    "    cv_mean_var_scores = []\n",
    "    c={}\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Dividir dados em treino e teste\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        features_num=X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "        features_cat=X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), features_num),\n",
    "            (OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    "            )\n",
    "        \n",
    "        # Aplicar preprocessamento\n",
    "        X_train_norm = preprocessor.fit_transform(X_train)\n",
    "        X_test_norm = preprocessor.transform(X_test)\n",
    "        \n",
    "        # Logaritmizar\n",
    "        y_train_log = np.log(y_train)\n",
    "        y_test_log = np.log(y_test)\n",
    "         \n",
    "        if n==1:\n",
    "            model=regressao_linear(X_train_norm,y_train_log)\n",
    "        elif n==2:\n",
    "            model=random_forest(X_train_norm,y_train_log)\n",
    "        elif n==3:\n",
    "            model=ada_boost(X_train_norm,y_train_log)\n",
    "        elif n==4:\n",
    "            model=decison_tree(X_train_norm,y_train_log)\n",
    "        else:\n",
    "            model=rn(X_train_norm,y_train_log,X_test_norm,y_test_log)\n",
    "            \n",
    "        if n == 5:\n",
    "            pred_log = model.predict(X_test_norm).flatten()\n",
    "        else:\n",
    "            pred_log = model.predict(X_test_norm)\n",
    "            \n",
    "        pred_orig_scale = np.exp(pred_log)\n",
    "\n",
    "        sorted_error, cumulative_freq = calculate_rec_curve(y_test, pred_orig_scale)\n",
    "        c[n] = (sorted_error, cumulative_freq)\n",
    "        \n",
    "        # Calcular as métricas\n",
    "        r2 = r2_score(np.exp(y_test_log), pred_orig_scale)\n",
    "        ev = explained_variance_score(np.exp(y_test_log), pred_orig_scale)\n",
    "        mae = mean_absolute_error(np.exp(y_test_log), pred_orig_scale)\n",
    "        mse = mean_squared_error(np.exp(y_test_log), pred_orig_scale)\n",
    "        msle = mean_squared_log_error(np.exp(y_test_log), pred_orig_scale)\n",
    "        mape = mean_absolute_percentage_error(np.exp(y_test_log), pred_orig_scale)\n",
    "        rec_auc_value = rec_auc(np.exp(y_test_log), pred_orig_scale)\n",
    "        #rec_curve = calculate_rec_curve(np.exp(y_test_log), pred_orig_scale)\n",
    "        coeff_var = coefficient_of_variation(np.exp(y_test_log), pred_orig_scale)\n",
    "        mean_var = mean_of_variation(np.exp(y_test_log), pred_orig_scale)\n",
    "\n",
    "        # Armazenar as métricas\n",
    "        cv_r2_scores.append(r2)\n",
    "        cv_ev_scores.append(ev)\n",
    "        cv_mae_scores.append(mae)\n",
    "        cv_mse_scores.append(mse)\n",
    "        cv_msle_scores.append(msle)\n",
    "        cv_mape_scores.append(mape)\n",
    "        cv_rec_auc_scores.append(rec_auc_value)\n",
    "        cv_coeff_var_scores.append(coeff_var)\n",
    "        cv_mean_var_scores.append(mean_var)\n",
    "\n",
    "# Calcular as médias e desvios padrão das métricas\n",
    "    metrics = {\n",
    "        \"R2 Score\": (np.mean(cv_r2_scores), np.std(cv_r2_scores)),\n",
    "        \"Explained Variance Score\": (np.mean(cv_ev_scores), np.std(cv_ev_scores)),\n",
    "        \"Mean Absolute Error\": (np.mean(cv_mae_scores), np.std(cv_mae_scores)),\n",
    "        \"Mean Squared Error\": (np.mean(cv_mse_scores), np.std(cv_mse_scores)),\n",
    "        \"Mean Squared Log Error\": (np.mean(cv_msle_scores), np.std(cv_msle_scores)),\n",
    "        \"Mean Absolute Percentage Error\": (np.mean(cv_mape_scores), np.std(cv_mape_scores)),\n",
    "        \"REC AUC\": (np.mean(cv_rec_auc_scores), np.std(cv_rec_auc_scores)),\n",
    "        \"Coeff. of Variation\": (np.mean(cv_coeff_var_scores), np.std(cv_coeff_var_scores)),\n",
    "        \"Mean of Variation\": (np.mean(cv_mean_var_scores), np.std(cv_mean_var_scores))\n",
    "    }\n",
    "     \n",
    "    return metrics, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regressão linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#cenario 1 -> com outlier\n",
    "def regressao_linear(X_train_norm,y_train_log):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_norm, y_train_log)\n",
    "    return model\n",
    "\n",
    "#modelo(1,X1,y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cenario 1 -> com outlier\n",
    "def random_forest(X_train_norm,y_train_log):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train_norm,y_train_log)\n",
    "    return model\n",
    "#modelo(2,X1,y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cenario 1 -> com outliers \n",
    "def ada_boost(X_train_norm,y_train_log):\n",
    "    model = AdaBoostRegressor()\n",
    "    model.fit(X_train_norm,y_train_log)\n",
    "    return model\n",
    "#modelo(3,X1,y1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cenario 1 -> com outliers \n",
    "def decison_tree(X_train_norm,y_train_log):\n",
    "    model = tree.DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "    model.fit(X_train_norm,y_train_log)\n",
    "    return model\n",
    "#modelo(4,X1,y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redes neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout + regularização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "# Cenario 1 -> com outliers \n",
    "def rn(X_train_norm,y_train_log,X_test_norm,y_test_log):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=32, activation='relu', input_dim=X_train_norm.shape[1]))\n",
    "    model.add(Dropout(0.3))  # Dropout para reduzir overfitting\n",
    "    model.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    model.fit(X_train_norm, y_train_log, epochs=50, batch_size=32, validation_data=(X_test_norm, y_test_log))\n",
    "    return model\n",
    "#modelo(5,X1,y1)\n",
    "\n",
    "#Plotando gráfico do histórico de treinamento\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(resultado.history['loss'])\n",
    "# plt.plot(resultado.history['val_loss'])\n",
    "# plt.title('Histórico de Treinamento')\n",
    "# plt.ylabel('Função de custo')\n",
    "# plt.xlabel('Épocas de treinamento')\n",
    "# plt.legend(['Erro treino', 'Erro teste'])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### greedy search -> otimização de parâmetros RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Função para construir o modelo Keras\n",
    "def create_model(units1=64, units2=32, dropout_rate=0.5, kernel_regularizer=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=units1, activation='relu', input_dim=X1_train_norm.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=units2, activation='relu', kernel_regularizer=regularizers.l2(kernel_regularizer)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Definindo os parâmetros a serem otimizados\n",
    "param_grid = {\n",
    "    'units1': [32, 64, 128],\n",
    "    'units2': [16, 32, 64],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'kernel_regularizer': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Realizando a pesquisa em grade\n",
    "grid_search_results = []\n",
    "\n",
    "for units1 in param_grid['units1']:\n",
    "    for units2 in param_grid['units2']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "            for kernel_regularizer in param_grid['kernel_regularizer']:\n",
    "                model = create_model(units1=units1, units2=units2, dropout_rate=dropout_rate, kernel_regularizer=kernel_regularizer)\n",
    "                history = model.fit(X1_train_norm, y1_train_log, epochs=100, batch_size=32, verbose=0, validation_split=0.2, callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "                val_loss = history.history['val_loss'][-1]\n",
    "                grid_search_results.append({\n",
    "                    'units1': units1,\n",
    "                    'units2': units2,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'kernel_regularizer': kernel_regularizer,\n",
    "                    'val_loss': val_loss\n",
    "                })\n",
    "\n",
    "# Encontrando os melhores parâmetros\n",
    "best_params = min(grid_search_results, key=lambda x: x['val_loss'])\n",
    "\n",
    "# Imprimindo os resultados\n",
    "print(\"Melhores parâmetros encontrados: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparação dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cenário 1 -> Com Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics_rl,c_rl=modelo(1,XX,y)\n",
    "metrics_rf, c_rf=modelo(2,XX,y)\n",
    "metrics_ada,c_ada=modelo(3,XX,y)\n",
    "metrics_dt,c_dt=modelo(4,XX,y)\n",
    "metrics_rn,c_rn=modelo(5,XX,y)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Random Forest\", \"AdaBoost\", \"Decision Tree\", \"Neural Network\"],\n",
    "    \"R2 Score\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"R2 Score\"][0], metrics_rl[\"R2 Score\"][1]),\n",
    "                 \"{:.2f} +- {:.2f}\".format(metrics_rf[\"R2 Score\"][0], metrics_rf[\"R2 Score\"][1]),\n",
    "                 \"{:.2f} +- {:.2f}\".format(metrics_ada[\"R2 Score\"][0], metrics_ada[\"R2 Score\"][1]),\n",
    "                 \"{:.2f} +- {:.2f}\".format(metrics_dt[\"R2 Score\"][0], metrics_dt[\"R2 Score\"][1]),\n",
    "                 \"{:.2f} +- {:.2f}\".format(metrics_rn[\"R2 Score\"][0], metrics_rn[\"R2 Score\"][1])],\n",
    "    \"Explained Variance Score\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"Explained Variance Score\"][0], metrics_rl[\"Explained Variance Score\"][1]),\n",
    "                                  \"{:.2f} +- {:.2f}\".format(metrics_rf[\"Explained Variance Score\"][0], metrics_rf[\"Explained Variance Score\"][1]),\n",
    "                                  \"{:.2f} +- {:.2f}\".format(metrics_ada[\"Explained Variance Score\"][0], metrics_ada[\"Explained Variance Score\"][1]),\n",
    "                                  \"{:.2f} +- {:.2f}\".format(metrics_dt[\"Explained Variance Score\"][0], metrics_dt[\"Explained Variance Score\"][1]),\n",
    "                                  \"{:.2f} +- {:.2f}\".format(metrics_rn[\"Explained Variance Score\"][0], metrics_rn[\"Explained Variance Score\"][1])],\n",
    "    \"Mean Absolute Error\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"Mean Absolute Error\"][0], metrics_rl[\"Mean Absolute Error\"][1]),\n",
    "                             \"{:.2f} +- {:.2f}\".format(metrics_rf[\"Mean Absolute Error\"][0], metrics_rf[\"Mean Absolute Error\"][1]),\n",
    "                             \"{:.2f} +- {:.2f}\".format(metrics_ada[\"Mean Absolute Error\"][0], metrics_ada[\"Mean Absolute Error\"][1]),\n",
    "                             \"{:.2f} +- {:.2f}\".format(metrics_dt[\"Mean Absolute Error\"][0], metrics_dt[\"Mean Absolute Error\"][1]),\n",
    "                             \"{:.2f} +- {:.2f}\".format(metrics_rn[\"Mean Absolute Error\"][0], metrics_rn[\"Mean Absolute Error\"][1])],\n",
    "    \"Mean Squared Error\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"Mean Squared Error\"][0], metrics_rl[\"Mean Squared Error\"][1]),\n",
    "                            \"{:.2f} +- {:.2f}\".format(metrics_rf[\"Mean Squared Error\"][0], metrics_rf[\"Mean Squared Error\"][1]),\n",
    "                            \"{:.2f} +- {:.2f}\".format(metrics_ada[\"Mean Squared Error\"][0], metrics_ada[\"Mean Squared Error\"][1]),\n",
    "                            \"{:.2f} +- {:.2f}\".format(metrics_dt[\"Mean Squared Error\"][0], metrics_dt[\"Mean Squared Error\"][1]),\n",
    "                            \"{:.2f} +- {:.2f}\".format(metrics_rn[\"Mean Squared Error\"][0], metrics_rn[\"Mean Squared Error\"][1])],\n",
    "    \"Mean Squared Log Error\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"Mean Squared Log Error\"][0], metrics_rl[\"Mean Squared Log Error\"][1]),\n",
    "                                \"{:.2f} +- {:.2f}\".format(metrics_rf[\"Mean Squared Log Error\"][0], metrics_rf[\"Mean Squared Log Error\"][1]),\n",
    "                                \"{:.2f} +- {:.2f}\".format(metrics_ada[\"Mean Squared Log Error\"][0], metrics_ada[\"Mean Squared Log Error\"][1]),\n",
    "                                \"{:.2f} +- {:.2f}\".format(metrics_dt[\"Mean Squared Log Error\"][0], metrics_dt[\"Mean Squared Log Error\"][1]),\n",
    "                                \"{:.2f} +- {:.2f}\".format(metrics_rn[\"Mean Squared Log Error\"][0], metrics_rn[\"Mean Squared Log Error\"][1])],\n",
    "    \"Mean Absolute Percentage Error\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"Mean Absolute Percentage Error\"][0], metrics_rl[\"Mean Absolute Percentage Error\"][1]),\n",
    "                                        \"{:.2f} +- {:.2f}\".format(metrics_rf[\"Mean Absolute Percentage Error\"][0], metrics_rf[\"Mean Absolute Percentage Error\"][1]),\n",
    "                                        \"{:.2f} +- {:.2f}\".format(metrics_ada[\"Mean Absolute Percentage Error\"][0], metrics_ada[\"Mean Absolute Percentage Error\"][1]),\n",
    "                                        \"{:.2f} +- {:.2f}\".format(metrics_dt[\"Mean Absolute Percentage Error\"][0], metrics_dt[\"Mean Absolute Percentage Error\"][1]),\n",
    "                                        \"{:.2f} +- {:.2f}\".format(metrics_rn[\"Mean Absolute Percentage Error\"][0], metrics_rn[\"Mean Absolute Percentage Error\"][1])],\n",
    "    \"REC AUC\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"REC AUC\"][0], metrics_rl[\"REC AUC\"][1]),\n",
    "                \"{:.2f} +- {:.2f}\".format(metrics_rf[\"REC AUC\"][0], metrics_rf[\"REC AUC\"][1]),\n",
    "                \"{:.2f} +- {:.2f}\".format(metrics_ada[\"REC AUC\"][0], metrics_ada[\"REC AUC\"][1]),\n",
    "                \"{:.2f} +- {:.2f}\".format(metrics_dt[\"REC AUC\"][0], metrics_dt[\"REC AUC\"][1]),\n",
    "                \"{:.2f} +- {:.2f}\".format(metrics_rn[\"REC AUC\"][0], metrics_rn[\"REC AUC\"][1])],\n",
    "    \"Coeff. of Variation\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"Coeff. of Variation\"][0], metrics_rl[\"Coeff. of Variation\"][1]),\n",
    "                            \"{:.2f} +- {:.2f}\".format(metrics_rf[\"Coeff. of Variation\"][0], metrics_rf[\"Coeff. of Variation\"][1]),\n",
    "                            \"{:.2f} +- {:.2f}\".format(metrics_ada[\"Coeff. of Variation\"][0], metrics_ada[\"Coeff. of Variation\"][1]),\n",
    "                            \"{:.2f} +- {:.2f}\".format(metrics_dt[\"Coeff. of Variation\"][0], metrics_dt[\"Coeff. of Variation\"][1]),\n",
    "                            \"{:.2f} +- {:.2f}\".format(metrics_rn[\"Coeff. of Variation\"][0], metrics_rn[\"Coeff. of Variation\"][1])],\n",
    "    \"Mean of Variation\": [\"{:.2f} +- {:.2f}\".format(metrics_rl[\"Mean of Variation\"][0], metrics_rl[\"Mean of Variation\"][1]),\n",
    "                          \"{:.2f} +- {:.2f}\".format(metrics_rf[\"Mean of Variation\"][0], metrics_rf[\"Mean of Variation\"][1]),\n",
    "                          \"{:.2f} +- {:.2f}\".format(metrics_ada[\"Mean of Variation\"][0], metrics_ada[\"Mean of Variation\"][1]),\n",
    "                          \"{:.2f} +- {:.2f}\".format(metrics_dt[\"Mean of Variation\"][0], metrics_dt[\"Mean of Variation\"][1]),\n",
    "                          \"{:.2f} +- {:.2f}\".format(metrics_rn[\"Mean of Variation\"][0], metrics_rn[\"Mean of Variation\"][1])]})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curva de REC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_curves = {**c_rl, **c_rf, **c_ada, **c_dt, **c_rn}\n",
    "\n",
    "# Plotar todas as curvas REC\n",
    "plot_rec_curves(all_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  # Importar também o matplotlib para controle adicional sobre os gráficos\n",
    "def modelo_classificacao(n,X,y):\n",
    "    bins = [0, 10000, 30000, 60000, 100000, float('inf')]\n",
    "    labels = ['Muito Baixo', 'Baixo', 'Médio', 'Alto', 'Luxo']\n",
    "    y = pd.cut(y, bins=bins, labels=labels)\n",
    "    y_true_all, y_pred_all,y_pred_all_original = [], [],[]\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Dividir dados em treino e teste\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        features_num=X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "        features_cat=X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), features_num),\n",
    "            (OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    "            )\n",
    "        \n",
    "        # Aplicar preprocessamento\n",
    "        X_train_norm = preprocessor.fit_transform(X_train)\n",
    "        X_test_norm = preprocessor.transform(X_test)\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train_norm, y_train)\n",
    "         \n",
    "        if n==6:\n",
    "            model=svm(X_train_smote,y_train_smote)\n",
    "        elif n==7:\n",
    "            model=decision_tree_cl(X_train_smote,y_train_smote)\n",
    "        elif n==8:\n",
    "            model=regressao_logistica(X_train_smote,y_train_smote)\n",
    "        elif n==9:\n",
    "            model=ada_boost_cl(X_train_smote,y_train_smote)\n",
    "        else:\n",
    "            model, y_test, le=rn_cl(X_train_smote,y_train_smote,X_test_norm,y_test)\n",
    "\n",
    "        y_pred = model.predict(X_test_norm)\n",
    "        y_pred_all_original.extend(y_pred)\n",
    "\n",
    "        if n==10:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "\n",
    "    if n==6:\n",
    "            y_test_binarized = label_binarize(y_true_all, classes=np.unique(y_true_all))\n",
    "            y_pred_binarized = label_binarize(y_pred_all, classes=np.unique(y_pred_all))\n",
    "            curva_roc(y_test_binarized ,y_pred_binarized,labels)\n",
    "    elif n==10:\n",
    "        y_test_cat = to_categorical(y_true_all)\n",
    "        curva_roc_rn(y_test_cat,y_pred_all_original,le)\n",
    "    else: \n",
    "        y_score = model.predict_proba(X_test_norm)\n",
    "        y_test_bin = label_binarize(y_test, classes=labels)\n",
    "        curva_roc(y_test_bin,y_score,labels)\n",
    "\n",
    "    repertorio=classification_report(y_true_all, y_pred_all, target_names=labels)\n",
    "\n",
    "    matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "    print(plot_confusion_matrix(matrix, labels))\n",
    "    \n",
    "    return print(repertorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def curva_roc(y_test_binarized ,y_pred_binarized,labels):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(labels)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_binarized[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plotando a curva ROC para cada classe\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(len(labels)):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'ROC curve (area = {roc_auc[i]:.2f}) for {labels[i]}')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Linha de não discriminação\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.title('Curva ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "\n",
    "def curva_roc_rn(y_test_cat, y_pred_nn, le):\n",
    "    y_test_cat = np.array(y_test_cat)\n",
    "    y_pred_nn = np.array(y_pred_nn)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = y_test_cat.shape[1]\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_cat[:, i], y_pred_nn[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Calcular a curva ROC média (macro)\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plotar todas as curvas ROC\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Linha de não discriminação\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple', 'blue', 'yellow', 'pink'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC classe {le.classes_[i]} (área = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle=':', linewidth=4, label=f'ROC média (macro) (área = {roc_auc[\"macro\"]:.2f})')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.title('Curva ROC para cada classe')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, labels):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Previsto')\n",
    "    plt.ylabel('Real')\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cenário 1 c/outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = subset_1['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm(X_train_smote, y_train_smote):\n",
    "    svm_model = SVC(random_state=42)\n",
    "    svm_model.fit(X_train_smote, y_train_smote)\n",
    "    return svm_model\n",
    "\n",
    "modelo_classificacao(6, X2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decison Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def decision_tree_cl(X_train_smote,y_train_smote):\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    return model\n",
    "modelo_classificacao(7,X2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def regressao_logistica(X_train_smote,y_train_smote):\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    return model\n",
    "modelo_classificacao(8,X2,y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net - classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def rn_cl(X_train_smote,y_train_smote,X_test_norm,y_test):\n",
    "# transformação numerica\n",
    "    le = LabelEncoder()\n",
    "    y_train_smote_num = le.fit_transform(y_train_smote)\n",
    "    y_test_num = le.transform(y_test)\n",
    "\n",
    "    #vetores binarios\n",
    "    y_train_smote_cat = to_categorical(y_train_smote_num)\n",
    "    y_test_cat = to_categorical(y_test_num)\n",
    "\n",
    "    # O mesmo que em regressão\n",
    "    def create_nn_model(input_dim, output_dim):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(output_dim, activation='softmax'))  # Para classificação multiclasse\n",
    "        return model\n",
    "\n",
    "    input_dim = X_train_smote.shape[1]\n",
    "    output_dim = y_train_smote_cat.shape[1]\n",
    "\n",
    "    nn_model = create_nn_model(input_dim, output_dim)\n",
    "    nn_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Treinar o modelo\n",
    "    nn_model.fit(X_train_smote, y_train_smote_cat, epochs=50, batch_size=32, validation_split=0.2)\n",
    "    return nn_model,y_test_num,le\n",
    "\n",
    "modelo_classificacao(10,X2,y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def ada_boost_cl(X_train_smote,y_train_smote):\n",
    "    ada_clf = AdaBoostClassifier(random_state=42)\n",
    "    ada_clf.fit(X_train_smote, y_train_smote)\n",
    "    return ada_clf\n",
    "modelo_classificacao(9,X2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Suponha que 'df' seja o DataFrame com os dados originais\n",
    "# Selecionar as features relevantes para clustering\n",
    "features_for_clustering = ['milage', 'model_year', 'engine', 'transmission', 'ext_col', 'int_col']\n",
    "\n",
    "# Aplicar K-means clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(df[features_for_clustering])\n",
    "\n",
    "# Preparar os dados para o modelo supervisionado\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinar um modelo de Random Forest usando as novas features\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predição e Avaliação\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Suponha que 'df' seja o DataFrame com os dados originais\n",
    "# Aplicar K-means clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(df[features_for_clustering])\n",
    "\n",
    "# Treinar um modelo para cada cluster\n",
    "models = {}\n",
    "for cluster in df['cluster'].unique():\n",
    "    cluster_data = df[df['cluster'] == cluster]\n",
    "    X_cluster = cluster_data.drop(['price', 'cluster'], axis=1)\n",
    "    y_cluster = cluster_data['price']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_cluster, y_cluster, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(f'Cluster {cluster} - RMSE: {rmse}')\n",
    "    \n",
    "    models[cluster] = model\n",
    "\n",
    "# Aplicar K-means clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(df[features_for_clustering])\n",
    "\n",
    "# Adicionar a distância ao centroide de cada cluster como nova feature\n",
    "df['distance_to_centroid'] = kmeans.transform(df[features_for_clustering]).min(axis=1)\n",
    "\n",
    "# Preparar os dados para o modelo supervisionado\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinar um modelo de Random Forest usando as novas features\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predição e Avaliação\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f'RMSE: {rmse}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
