{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', None)\n",
    "data=pd.read_csv('train.csv') # dataset do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dtypes\n",
    "data.info() #milage\n",
    "# data.head()\n",
    "# data.tail()\n",
    "# data.shape #(3207, 12)\n",
    "# data.nunique() # valores unicos para cada coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo mais antigo e mais novo:\n",
    "antigo=data['model_year'].min()\n",
    "novo=data['model_year'].max()\n",
    "print(antigo,novo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correção: 'milage' -> INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#milage:\n",
    "valores_corrigidos=[]\n",
    "for milage in data['milage']: # para cada quilometragem do dataset\n",
    "    separa=re.split(r'[ ]',milage) # sepagar o numero do mi\n",
    "    numero=separa[0] # numero\n",
    "    letra=separa[1] # mi\n",
    "    verifica=re.search(r'^[0-9]+[,]?[0-9]+$',numero) # verifica se o numeros sao sempre iguais(com casas decimais ou não)\n",
    "    verifica2=re.search(r'^mi\\.$',letra) # verifica se a letra é sempre mi\n",
    "    if verifica and verifica2: # se seguir o padra numero + mi:\n",
    "        numero=int(re.sub(r',','',numero)) # retira a , dos numeros e passa para inteiro ( estavam em obj)\n",
    "        valores_corrigidos.append(numero)\n",
    "    else: \n",
    "        print(milage,False) # tem dados diferentes no dataset\n",
    "\n",
    "data['milage']=valores_corrigidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificaçao\n",
    "for m in data['milage']:\n",
    "    if not isinstance(m, (int)):\n",
    "        print('dado incorreto')\n",
    "# tudo certo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NULL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuel_type'].value_counts() # –  38 ( existem 38 linhas com (-) -> nao se sabe)\n",
    "data['accident'].value_counts()\n",
    "data['clean_title'].value_counts() # Yes -> 2740, os restantes são valores nulos (nan)\n",
    "# data['clean_title'].unique()\n",
    "# verificar outros tipos de dizer valores nulos: ( como (-) por exemplo)\n",
    "data['brand'].value_counts() # tudo certo\n",
    "data['model'].nunique() # tudo certo\n",
    "data['model_year'].value_counts() # tudo certo\n",
    "data['engine'].value_counts() # – 38 -> nao se sabe\n",
    "data['transmission'].value_counts() # 4 -> nao se sabe \n",
    "# data['ext_col'].value_counts()# 11 -> nao se sabe\n",
    "# data['int_col'].value_counts() # 98 -> nao se sabe\n",
    "# for preco in data['price']: # tudo certo\n",
    "#     if not isinstance(preco, int):\n",
    "#         print('erro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['milage'])\n",
    "# len(data['milage'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --> nº de velocidades na transmissão existentes no dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2=list(data['transmission'].unique())\n",
    "alls = [int(numero) for string in d2 for numero in re.findall(r'\\d+', string)]\n",
    "list(set(alls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verificar significado de '-' e NaN para o tipo de combustível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.loc[data['fuel_type'] == '–'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['fuel_type'] == '–') & (data['engine'] != '–')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OS CARROS PARA OS QUAIS O FUEL_TYPE É '-' NÃO INDICADO, TAMBÉM NÃO SE CONHECE A CONFIGURAÇÃO DO MOTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAR QUE OS CARROS COM NAN NO TIPO DE COMBUSTÍVEL SÃO CARROS ELÉTRICOS\n",
    "elec=data[data['fuel_type'].isnull()]\n",
    "pattern = re.compile(r'\\bElectric\\b', flags=re.IGNORECASE)\n",
    "contains_electric = elec['engine'].str.contains(pattern, na=False)\n",
    "# Selecionar todas as linhas que não contêm 'Electric' na coluna 'engine'\n",
    "elec[~contains_electric]\n",
    "# elec[contains_electric].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### os carros com NaN para 'fuel_type' são carros elétricos\n",
    "Tesla --> elétrico\n",
    "Standard Range Battery --> elétrico\n",
    "111.2Ah / FR 70kW / RR 160kW (697V) --> especificidade de baterias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOXPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='box',figsize=(15,6),subplots=True) # grafico do codigo acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['model_year']<1990] # outlier do 1º gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['milage']>350000] # outlier do 2º gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['price']>1500000] # outlier 3º gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BARPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Supondo que 'data' seja seu DataFrame com colunas de atributos e 'preco'\n",
    "# Vamos calcular a média do preço para cada atributo (exceto a última coluna)\n",
    "data['fuel_type'].fillna('Elétrico', inplace=True)\n",
    "\n",
    "# Lista para armazenar os gráficos gerados\n",
    "colunas = ['brand', 'model', 'model_year', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
    "num_linhas = 3\n",
    "num_colunas = 3\n",
    "\n",
    "fig, axs = plt.subplots(num_linhas, num_colunas, figsize=(15, 10))\n",
    "\n",
    "# Iterar sobre as colunas do DataFrame\n",
    "for i, column in enumerate(colunas[:-1]):\n",
    "    # Calcular a média do preço para cada valor único na coluna\n",
    "    med = data.groupby(column)['price'].mean()\n",
    "    top = med.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    # Truncate long labels and append ellipsis\n",
    "    truncated_labels = [str(val)[:10] + '...' if len(str(val)) > 10 else str(val) for val in top.index]\n",
    "\n",
    "    # Determine the subplot index\n",
    "    linha = i // num_colunas\n",
    "    coluna = i % num_colunas\n",
    "\n",
    "    # Plotar o gráfico de barras para a média do preço por valor\n",
    "    axs[linha, coluna].bar(truncated_labels, top.values, color='deepskyblue')\n",
    "    axs[linha, coluna].set_title(f'Média de Preço por {column.upper()}')\n",
    "    axs[linha, coluna].set_ylabel('Média de Preço')\n",
    "    axs[linha, coluna].tick_params(axis='x', rotation=45)  # Rotacionar rótulos do eixo x\n",
    "\n",
    "# Ajustar o layout para evitar sobreposição\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### análise dos modelos com preço mais altos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data.sort_values(by='price', ascending=False).head(10)\n",
    "d[['brand', 'model', 'price']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LINEPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.groupby('milage')['price'].mean()\n",
    "d=d.sort_index()\n",
    "plt.scatter(d.index, d.values, marker='o', linestyle='-')\n",
    "plt.title('Preço Médio em Função do Número de Quilômetros')\n",
    "plt.xlabel('Quilometragem')\n",
    "plt.ylabel('Preço Médio')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()  # Ajuste automático da disposição para evitar sobreposição\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### o preço diminui a medida que o nº de quilometros aumenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = data.groupby(['brand', 'model_year'])['price'].describe()\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ENGINE] novos atributos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Potencia'] = data['engine'].str.extract(r'(\\d+\\.\\d+)HP?')\n",
    "data['Capacidade_Motor'] = data['engine'].str.extract(r'(\\d+\\.\\d+|\\d+)\\s*(?:L|Liter)')\n",
    "data['Numero_Cilindros'] = data['engine'].str.extract(r'(?:V(\\d+)|I-(\\d+)|I(\\d+)|(\\d+) Cylinder)').apply(lambda x: next(filter(lambda y: pd.notna(y), x), None), axis=1)\n",
    "# data['Tipo_Combustivel'] = data['engine'].str.extract(r'(Gasoline Fuel|Flexible Fuel|Electric)')\n",
    "data['Numero_Valvulas'] = data['engine'].str.extract(r'( \\d+)V')\n",
    "\n",
    "data['Potencia'] = pd.to_numeric(data['Potencia'], errors='coerce')\n",
    "data['Capacidade_Motor'] = pd.to_numeric(data['Capacidade_Motor'], errors='coerce')\n",
    "data['Numero_Cilindros'] = pd.to_numeric(data['Numero_Cilindros'], errors='coerce')\n",
    "data['Numero_Valvulas'] = pd.to_numeric(data['Numero_Valvulas'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=len(data[data['Numero_Valvulas'].isnull()])\n",
    "d\n",
    "# d[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(50)\n",
    "# data[['engine', 'Potencia', 'Capacidade_Motor',  'Numero_Cilindros',  'Numero_Valvulas' ]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BARPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame with columns of attributes and 'price'\n",
    "# Let's calculate the mean price for each attribute (except the last column)\n",
    "\n",
    "# List to store the generated plots\n",
    "columns = ['Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas']\n",
    "\n",
    "num_linhas = 2\n",
    "num_colunas = 2\n",
    "\n",
    "fig, axs = plt.subplots(num_linhas, num_colunas, figsize=(10, 5))\n",
    "\n",
    "# Iterar sobre as colunas do DataFrame\n",
    "for i, column in enumerate(columns):\n",
    "    # Calcular a média do preço para cada valor único na coluna\n",
    "    med = data.groupby(column)['price'].mean()\n",
    "    top = med.sort_values(ascending=False).head(13)\n",
    "    \n",
    "    # Truncate long labels and append ellipsis\n",
    "    truncated_labels = [str(val)[:10] + '...' if len(str(val)) > 10 else str(val) for val in top.index]\n",
    "\n",
    "    # Determine the subplot index\n",
    "    linha = i // num_colunas\n",
    "    coluna = i % num_colunas\n",
    "\n",
    "    # Plotar o gráfico de barras para a média do preço por valor\n",
    "    axs[linha, coluna].bar(truncated_labels, top.values, color='blueviolet')\n",
    "    axs[linha, coluna].set_title(f'Média de Preço por {column.upper()}')\n",
    "    axs[linha, coluna].set_ylabel('Média de Preço')\n",
    "    axs[linha, coluna].tick_params(axis='x', rotation=45)  # Rotacionar rótulos do eixo x\n",
    "\n",
    "# Ajustar o layout para evitar sobreposição\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[data['Numero_Valvulas'] == 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Numero_Valvulas'].unique())\n",
    "sorted(list(data['Numero_Valvulas'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Numero_Cilindros'].unique())\n",
    "list(sorted(data['Numero_Cilindros'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data['Capacidade_Motor'].unique())\n",
    "#sorted(list(data['Capacidade_Motor'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(data['Potencia'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verificação de possíveis valores únicos reportados pela análise do boxplot preço / marca no R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['brand'] == 'Maybach'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('train_ccols.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SUBSETS] com dataset W/ ENGINE caract-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', None)\n",
    "p=pd.read_csv('train_ccols.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituir pela mediana :\n",
    "- num valvulas\n",
    "- num cilindros\n",
    "- potencia\n",
    "- capacidade_motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = p['Numero_Valvulas'].median()\n",
    "m2 = p['Numero_Cilindros'].median()\n",
    "m3 = p['Potencia'].median()\n",
    "m4 = p['Capacidade_Motor'].median()\n",
    "\n",
    "p['Numero_Valvulas'].fillna(m, inplace=True)\n",
    "p['Numero_Cilindros'].fillna(m2, inplace=True)\n",
    "p['Potencia'].fillna(m3, inplace=True)\n",
    "p['Capacidade_Motor'].fillna(m4, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['clean_title'] = p['clean_title'].fillna('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### novos atributos (derivações)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "p['T2'] = p['transmission'].str.extract(r'(\\d+)')\n",
    "p['T2'] = pd.to_numeric(p['T2'], errors='coerce')\n",
    "m5 = p['T2'].mean() \n",
    "p['T2'].fillna(round(m5), inplace=True)\n",
    "#p.head()\n",
    "#p['T2'].isnull().sum()\n",
    "# p['T2'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria_ET (engine + transmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = p[['Potencia', 'Capacidade_Motor', 'Numero_Cilindros', 'Numero_Valvulas', 'T2']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "n_clusters = 5\n",
    "\n",
    "# Aplicar o algoritmo K-means para agrupar os carros em clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Adicionar uma nova coluna 'Categoria' ao DataFrame com base nos clusters\n",
    "p['Categoria_ET'] = kmeans.labels_\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contagem de amostras em cada categoria\n",
    "categoria_counts = p['Categoria_ET'].value_counts().sort_index()\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "plt.bar(categoria_counts.index, categoria_counts.values)\n",
    "\n",
    "# Adicionar rótulos e título\n",
    "plt.xlabel('Categoria_ET')\n",
    "plt.ylabel('Número de amostras')\n",
    "plt.title('Distribuição das Categorias')\n",
    "\n",
    "#Mostrar o gráfico\n",
    "plt.show()\n",
    "\n",
    "p['Categoria_ET'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Categoria_ET'] = p['Categoria_ET'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria Marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = p['brand'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência Marca'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência Marca'\n",
    "    else:\n",
    "        return 'Média Frequência Marca'\n",
    "\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "p['Categoria_Marca'] = p['brand'].map(frequencias.apply(categorizar))\n",
    "\n",
    "p['Categoria_Marca'] = p['Categoria_Marca'].astype('category')\n",
    "p['Categoria_Marca'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = p['model'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência Modelo'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência Modelo'\n",
    "    else:\n",
    "        return 'Média Frequência Modelo'\n",
    "\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "p['Categoria_Modelo'] = p['model'].map(frequencias.apply(categorizar))\n",
    "p['Categoria_Modelo'] = p['Categoria_Modelo'].astype('category')\n",
    "p['Categoria_Modelo'].value_counts()\n",
    "p['Categoria_Modelo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalidade das novas colunas (numéricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Lista das colunas numéricas que você deseja analisar\n",
    "colunas_numericas = ['Potencia', 'Capacidade_Motor', 'Numero_Cilindros',\n",
    "       'Numero_Valvulas', 'T2']\n",
    "\n",
    "num_colunas = len(colunas_numericas)\n",
    "fig, axs = plt.subplots(2, num_colunas, figsize=(15, 8))\n",
    "print(f'Teste de Shapiro-Wilk: \\n')\n",
    "\n",
    "# Iterar sobre as colunas numéricas e plotar os gráficos em cada subplot\n",
    "for i, coluna in enumerate(colunas_numericas):\n",
    "    # Histograma\n",
    "    sns.histplot(p[coluna], kde=True, ax=axs[0, i])\n",
    "    axs[0, i].set_title(f'{coluna}')\n",
    "    \n",
    "    # Gráfico QQ\n",
    "    stats.probplot(p[coluna], dist=\"norm\", plot=axs[1, i])\n",
    "    axs[1, i].set_title(f'QQ-plot')\n",
    " \n",
    "    # Teste de Shapiro-Wilk\n",
    "    stat, pv = stats.shapiro(p[coluna])\n",
    "    print(f'{coluna}:')\n",
    "    print(f'Valor p: {pv}')\n",
    "    if pv > 0.05:\n",
    "        print('Não podemos rejeitar a hipótese nula - A distribuição parece normal.')\n",
    "    else:\n",
    "        print('Rejeita-se a hipótese nula -> a distribuição não segue o modelo normal.')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como se pode observar, nenhum dos novos atributos (numéricos possuem distribuição normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Substituição de valores nulos e '-' PELA MODA\n",
    "- Remoção outliers LOF\n",
    "- Variavies cat -> numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL's substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['fuel_type'].unique()\n",
    "data1['fuel_type'].isnull().sum()\n",
    "data1['fuel_type'].fillna('Eletric', inplace=True)\n",
    "mod = data1['fuel_type'].mode()[0]\n",
    "\n",
    "data1['fuel_type'] = data1['fuel_type'].replace('–', mod)\n",
    "data1['fuel_type'].unique()\n",
    "\n",
    "mod1=data1['accident'].mode()[0]\n",
    "data1['accident'].fillna(mod1, inplace=True)\n",
    "\n",
    "mod3=data1['engine'].mode()[0]\n",
    "data1['engine'] = data1['engine'].replace('–', mod3)\n",
    "\n",
    "mod5=data1['ext_col'].mode()[0]\n",
    "data1['ext_col'] = data1['ext_col'].replace('–', mod5)\n",
    "\n",
    "mod6=data1['int_col'].mode()[0]\n",
    "data1['int_col'] = data1['int_col'].replace('–', mod6)\n",
    "\n",
    "mod7=data1['transmission'].mode()[0]\n",
    "data1['transmission'] = data1['transmission'].replace('–', mod6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data1['int_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    elif limite_baixa < frequencia < limite_alta:  # Verifica se a frequência está entre os limites\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data1['Categoria_IntCol'] = data1['int_col'].map(frequencias.apply(categorizar))\n",
    "data1['Categoria_IntCol'] = data1['Categoria_IntCol'].astype('category')\n",
    "data1['Categoria_IntCol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['ext_col'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categoria cor externa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data1['ext_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    elif limite_baixa < frequencia < limite_alta:  # Verifica se a frequência está entre os limites\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data1['Categoria_ExtCol'] = data1['ext_col'].map(frequencias.apply(categorizar))\n",
    "data1['Categoria_ExtCol'] = data1['Categoria_ExtCol'].astype('category')\n",
    "data1['Categoria_ExtCol'].value_counts()\n",
    "data1['Categoria_ExtCol'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Selecionar apenas as variáveis numéricas\n",
    "numeric_df = data1.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calcular a correlação de Spearman\n",
    "spearman_corr = numeric_df.corr(method='spearman')\n",
    "\n",
    "# Visualizar a matriz de correlação de Spearman\n",
    "print(\"Matriz de correlação de Spearman:\")\n",
    "print(spearman_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#va's categóricas\n",
    "categorical_columns = data1.select_dtypes(include=['object','category']).columns\n",
    "correlations = {}\n",
    "for column in categorical_columns:\n",
    "    corr, _ = spearmanr(data1[column], data1['price'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier por IQR -> V.a Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict = {}\n",
    "\n",
    "# Variáveis Numéricas\n",
    "for column in data1.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    Q1 = data1[column].quantile(0.25)\n",
    "    Q3 = data1[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outlier_conditional = ((data1[column] > Q1 - 1.5*IQR) & (data1[column] < Q3 + 1.5*IQR))\n",
    "\n",
    "    outlier_indices_dict[column] = data1.loc[~outlier_conditional].index\n",
    "    \n",
    "    num_outliers = len(data1[~outlier_conditional])\n",
    "    print(f\"Número de outliers em '{column}': {num_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### O número de válvulas têm muito pouca correlação com o preço -> N utiliza-se essa coluna para os cenários!\n",
    "###### Remove-se os 6 registros da capacidade do motor e os 57 com num de cilindros e 376 do T2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover registros com outliers (cujo correlação baixa com preço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict['Capacidade_Motor']\n",
    "outlier_indices_dict['Numero_Cilindros']\n",
    "outlier_indices_dict['T2']\n",
    "\n",
    "data1_s_out = data1.copy()\n",
    "\n",
    "outlier_indices_to_drop = outlier_indices_dict['Capacidade_Motor'].union(outlier_indices_dict['Numero_Cilindros']).union(outlier_indices_dict['T2'])\n",
    "data1_s_out.drop(outlier_indices_to_drop, inplace=True)\n",
    "\n",
    "# Verifica o tamanho do novo conjunto de dados\n",
    "print(\"Número de registros no novo conjunto de dados sem outliers:\", len(data1_s_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('subset_1.csv', index=False) # sem normalizar com outliers\n",
    "data1_s_out.to_csv('subset_1_s_out.csv', index=False) # sem normalizar sem outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  substituir '-' por desconhecido\n",
    "- Remoção outliers LOF\n",
    "- Variavies cat -> numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL's substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuel Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['fuel_type'].unique()\n",
    "data2['fuel_type'].isnull().sum()\n",
    "data2['fuel_type'].fillna('Eletric', inplace=True)\n",
    "data2['fuel_type'] = data2['fuel_type'].replace('–', 'desconhecido')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accident + engine + transmission + ex_col + int_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['accident'].fillna('desconhecido', inplace=True)\n",
    "data2['engine'] = data2['engine'].replace('–', 'desconhecido')\n",
    "data2['transmission'] = data2['transmission'].replace('–', 'desconhecido')\n",
    "data2['ext_col'] = data2['ext_col'].replace('–', 'desconhecido')\n",
    "data2['int_col'] = data2['int_col'].replace('–', 'desconhecido')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor interna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data2['int_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência int_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência int_col'\n",
    "    else:\n",
    "        return 'Média Frequência int_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data2['Categoria_IntCol'] = data2['int_col'].map(frequencias.apply(categorizar))\n",
    "data2['Categoria_IntCol'] = data2['Categoria_IntCol'].astype('category')\n",
    "data2['Categoria_IntCol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoria cor externa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = data2['ext_col'].value_counts()\n",
    "\n",
    "# Define os limites das categorias\n",
    "limite_alta = frequencias.quantile(0.75)\n",
    "limite_baixa = frequencias.quantile(0.25)\n",
    "\n",
    "# Função para atribuir categoria com base na frequência\n",
    "def categorizar(frequencia):\n",
    "    if frequencia >= limite_alta:\n",
    "        return 'Alta Frequência ext_col'\n",
    "    elif frequencia <= limite_baixa:\n",
    "        return 'Baixa Frequência ext_col'\n",
    "    else:\n",
    "        return 'Média Frequência ext_col'\n",
    "\n",
    "# Adiciona uma nova coluna 'categoria' ao DataFrame com as categorias das marcas\n",
    "data2['Categoria_ExtCol'] = data2['ext_col'].map(frequencias.apply(categorizar))\n",
    "data2['Categoria_ExtCol'] = data2['Categoria_ExtCol'].astype('category')\n",
    "data2['Categoria_ExtCol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Selecionar apenas as variáveis numéricas\n",
    "numeric_df = data2.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calcular a correlação de Spearman\n",
    "spearman_corr = numeric_df.corr(method='spearman')\n",
    "\n",
    "# Visualizar a matriz de correlação de Spearman\n",
    "print(\"Matriz de correlação de Spearman:\")\n",
    "print(spearman_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#va categorica\n",
    "categorical_columns = data2.select_dtypes(include=['object','category']).columns\n",
    "correlations = {}\n",
    "for column in categorical_columns:\n",
    "    corr, _ = spearmanr(data2[column], data2['price'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier por IQR -> va.'s Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis Numéricas\n",
    "for column in data2.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    Q1 = data2[column].quantile(0.25)\n",
    "    Q3 = data2[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    outlier_conditional = ((data2[column] > Q1 - 1.5*IQR) & (data2[column] < Q3 + 1.5*IQR))\n",
    "    outlier_indices_dict[column] = data2.loc[~outlier_conditional].index\n",
    "    \n",
    "    num_outliers = len(data2[~outlier_conditional])\n",
    "    print(f\"Número de outliers em '{column}': {num_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover registros com outliers (cuja correlação baixa com preço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices_dict['Capacidade_Motor']\n",
    "outlier_indices_dict['Numero_Cilindros']\n",
    "outlier_indices_dict['T2']\n",
    "\n",
    "data2_s_out = data2.copy()\n",
    "\n",
    "outlier_indices_to_drop = outlier_indices_dict['Capacidade_Motor'].union(outlier_indices_dict['Numero_Cilindros']).union(outlier_indices_dict['T2'])\n",
    "data2_s_out.drop(outlier_indices_to_drop, inplace=True)\n",
    "\n",
    "# Verifica o tamanho do novo conjunto de dados\n",
    "print(\"Número de registros no novo conjunto de dados sem outliers:\", len(data2_s_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('subset_2.csv', index=False) # sem normalizar com outliers\n",
    "data2_s_out.to_csv('subset_2_s_out.csv', index=False) # sem normalizar sem outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELAÇÃO\n",
    "### Como subset 1 é muito parecido com subset 2, só vamos utilizar o subset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "subset_1=pd.read_csv('subset_1.csv')\n",
    "subset_1_s_out=pd.read_csv('subset_1_s_out.csv')\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 1 -> todas as variáveis (sem categorias extras (redundância))\n",
    "#### - (se for usar todas as variáveis não da para normalizar os dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = subset_1[['brand','model','model_year','milage','fuel_type','engine','transmission','ext_col','int_col','accident','clean_title']]\n",
    "y1 = subset_1['price'] \n",
    "# definir amostra treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label logaritmizada (preço) + normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X1.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X1.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    ")\n",
    "\n",
    "# normalizar\n",
    "X1_train_norm = preprocessor.fit_transform(X1_train)\n",
    "X1_test_norm = preprocessor.transform(X1_test)\n",
    "\n",
    "# logaritmizar\n",
    "y1_train_log = np.log(y1_train)\n",
    "y1_test_log = np.log(y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sem outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1out = subset_1_s_out[['brand','model','model_year','milage','fuel_type','engine','ext_col','int_col','accident','clean_title']] # tinha T2 » retirei...\n",
    "y1out = subset_1_s_out['price'] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1out_train, X1out_test, y1out_train, y1out_test = train_test_split(X1out, y1out, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalizados e label logaritmizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X1out.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X1out.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    ")\n",
    "\n",
    "# normalizar\n",
    "X1out_train_norm = preprocessor.fit_transform(X1out_train) \n",
    "X1out_test_norm = preprocessor.transform(X1out_test)\n",
    "# logaritmizar\n",
    "y1out_train_log = np.log(y1out_train) # »»» mudei y1_train para y1out_train «««\n",
    "y1out_test_log = np.log(y1out_test) # »»» y1_train para y1out_test «««"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cenário 2 (todas as variáveis (categorias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = subset_1[['model_year', 'milage', 'fuel_type', 'accident', 'clean_title', 'Categoria_ET', 'Categoria_Marca', 'Categoria_Modelo', 'Categoria_IntCol', 'Categoria_ExtCol']]\n",
    "y2 = subset_1['price'] \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalização + label logaritmizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X2.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X2.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "X2_train_norm = preprocessor.fit_transform(X2_train)\n",
    "X2_test_norm = preprocessor.transform(X2_test)\n",
    "y2_train_log = np.log(y2_train)\n",
    "y2_test_log = np.log(y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sem outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2out = subset_1_s_out[['model_year','milage','fuel_type','accident','clean_title','Categoria_ET','Categoria_Marca','Categoria_Modelo','Categoria_IntCol','Categoria_ExtCol']]  \n",
    "y2out = subset_1_s_out['price'] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X2out_train, X2out_test, y2out_train, y2out_test = train_test_split(X2out, y2out, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X2out.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X2out.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "# normalizar\n",
    "X2out_train_norm = preprocessor.fit_transform(X2out_train) \n",
    "X2out_test_norm = preprocessor.transform(X2out_test)\n",
    "# logaritmizar\n",
    "y2out_train_log = np.log(y2out_train) # »»» mudei y1_train para y1out_train «««\n",
    "y2out_test_log = np.log(y2out_test) # »»» y1_train para y1out_test «««"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 3 (ao olho, características do carro novo sem influência do condutor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3=subset_1[['Potencia','Capacidade_Motor','Numero_Cilindros','T2']]\n",
    "y3=subset_1['price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalização + label logaritmizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X3.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X3.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "# normalizar\n",
    "X3_train_norm = preprocessor.fit_transform(X3_train)\n",
    "\n",
    "X3_test_norm = preprocessor.transform(X3_test)\n",
    "\n",
    "# logaritmizar\n",
    "y3_train_log = np.log(y3_train)\n",
    "y3_test_log = np.log(y3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sem outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3out = subset_1_s_out[['Potencia','Capacidade_Motor','Numero_Cilindros','T2']] \n",
    "y3out = subset_1_s_out['price'] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X3out_train, X3out_test, y3out_train, y3out_test = train_test_split(X3out, y3out, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X3out.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X3out.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(), features_cat),\n",
    ")\n",
    "\n",
    "# normalizar\n",
    "X3out_train_norm = preprocessor.fit_transform(X3out_train)\n",
    "\n",
    "X3out_test_norm = preprocessor.transform(X3out_test)\n",
    "\n",
    "# logaritmizar\n",
    "y3out_train_log = np.log(y3out_train)\n",
    "y3out_test_log = np.log(y3out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 4 (características do carro com influência do condutor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### com outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4=subset_1[['brand','model','model_year','milage','fuel_type','ext_col','int_col','accident','clean_title']]\n",
    "y4=subset_1['price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalização - label logaritmizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X4.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X4.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    ")\n",
    "\n",
    "# normalizar\n",
    "X4_train_norm = preprocessor.fit_transform(X4_train)\n",
    "\n",
    "X4_test_norm = preprocessor.transform(X4_test)\n",
    "\n",
    "# logaritmizar\n",
    "y4_train_log = np.log(y4_train)\n",
    "y4_test_log = np.log(y4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sem outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4out = subset_1_s_out[['brand','model','model_year','milage','fuel_type','ext_col','int_col','accident','clean_title']]\n",
    "y4out = subset_1_s_out['price'] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X4out_train, X4out_test, y4out_train, y4out_test = train_test_split(X4out, y4out, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalização + label logaritmizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "features_num=X4out.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "features_cat=X4out.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    ")\n",
    "\n",
    "# normalizar\n",
    "X4out_train_norm = preprocessor.fit_transform(X4out_train)\n",
    "\n",
    "X4out_test_norm = preprocessor.transform(X4out_test)\n",
    "\n",
    "# logaritmizar\n",
    "y4out_train_log = np.log(y4out_train)\n",
    "y4out_test_log = np.log(y4out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# previsões para os cenários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar Data / Treinar e Avaliar Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "# from sklearn import datasets\n",
    "from slickml.metrics import (\n",
    "    RegressionMetrics,\n",
    ")  # downloaded from https://github.com/slickml/slick-ml # btw pip install slickml\n",
    "from matplotlib import pyplot as plt\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X1_train:\", X1_train.shape)\n",
    "print(\"Shape of X1_test:\", X1_test.shape)\n",
    "print(\"Shape of y1_train:\", y1_train.shape)\n",
    "print(\"Shape of y1_test:\", y1_test.shape)\n",
    "# com log\n",
    "print(\"Shape of y1_train_log:\", y1_train_log.shape)\n",
    "print(\"Shape of y1_test_log:\", y1_test_log.shape)\n",
    "\n",
    "# os tests estão com 642 rows\n",
    "# treino com 2565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "model = LinearRegression()\n",
    "\n",
    "# Cenario 1 -> com outliers\n",
    "# model.fit(X1_train_norm, y1_train) # sem log\n",
    "# model.fit(X1_train_norm, y1_train_log)\n",
    "# Cenario 1 -> sem outliers\n",
    "# model.fit(X1out_train_norm, y1out_train)\n",
    "#model.fit(X1out_train_norm, y1out_train_log)\n",
    "\n",
    "# Cenario 2 -> com outliers\n",
    "#model.fit(X2_train_norm, y2_train_log)\n",
    "# Cenario 2 -> sem outlier\n",
    "#model.fit(X2out_train_norm, y2out_train_log)\n",
    "\n",
    "# Cenario 3 -> com outliers\n",
    "#model.fit(X3_train_norm, y3_train_log)\n",
    "# Cenario 3 -> sem outlier\n",
    "#model.fit(X3out_train_norm, y3out_train_log)\n",
    "\n",
    "# Cenario 4 -> com outliers\n",
    "#model.fit(X4_train_norm, y4_train_log)\n",
    "# Cenario 4 -> sem outlier\n",
    "model.fit(X4out_train_norm, y4out_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT \n",
    "\n",
    "# Cenario 1 -> com outliers\n",
    "# pred0 = model.predict(X1_test_norm)\n",
    "# Cenario 1 -> sem outliers\n",
    "#pred0 = model.predict(X1out_test_norm)\n",
    "\n",
    "# cenario 2 -> com outliers\n",
    "#pred0 = model.predict(X2_test_norm)\n",
    "# cenario 2 -> sem outliers\n",
    "#pred0 = model.predict(X2out_test_norm)\n",
    "\n",
    "# cenario 3 -> com outliers\n",
    "#pred0 = model.predict(X3_test_norm)\n",
    "# cenario 3 -> sem outliers\n",
    "#pred0 = model.predict(X3out_test_norm)\n",
    "\n",
    "# cenario 4 -> com outliers\n",
    "#pred0 = model.predict(X4_test_norm)\n",
    "# cenario 4 -> sem outliers\n",
    "pred0 = model.predict(X4out_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverter logaritmo dos valores preditos\n",
    "pred0_orig_scale = np.exp(pred0)\n",
    "\n",
    "# cenario 1 -> com outliers\n",
    "#reg_metrics0 = RegressionMetrics(np.exp(y1out_test_log), pred0_orig_scale)\n",
    "# reg_metrics0 = RegressionMetrics(y1_test, pred0)\n",
    "\n",
    "# cenario 2 -> com outliers\n",
    "#reg_metrics0 = RegressionMetrics(np.exp(y2_test_log), pred0_orig_scale)\n",
    "# cenario 2 -> com outliers\n",
    "#reg_metrics0 = RegressionMetrics(np.exp(y2out_test_log), pred0_orig_scale)\n",
    "\n",
    "# cenario 3 -> com outliers\n",
    "#reg_metrics0 = RegressionMetrics(np.exp(y3_test_log), pred0_orig_scale)\n",
    "# cenario 3 -> sem outliers\n",
    "#reg_metrics0 = RegressionMetrics(np.exp(y3out_test_log), pred0_orig_scale)\n",
    "\n",
    "# cenario 4 -> com outliers\n",
    "#reg_metrics0 = RegressionMetrics(np.exp(y4_test_log), pred0_orig_scale)\n",
    "# cenario 4 -> sem outliers\n",
    "reg_metrics0 = RegressionMetrics(np.exp(y4out_test_log), pred0_orig_scale)\n",
    "\n",
    "reg_metrics0.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict with Random Forest Regressor\n",
    "RF = RandomForestRegressor()\n",
    "\n",
    "# Cenario 1 -> com outliers\n",
    "# RF.fit(X1_train_norm, y1_train_log)\n",
    "# RF.fit(X1_train_norm, y1_train) # sem log\n",
    "# pred = RF.predict(X1_test_norm)\n",
    "\n",
    "# Cenario 1 -> sem outliers\n",
    "# RF.fit(X1out_train_norm, y1out_train) # sem log\n",
    "#RF.fit(X1out_train_norm, y1out_train_log)\n",
    "#pred = RF.predict(X1out_test_norm)\n",
    "\n",
    "# # Cenario 2 -> com outliers\n",
    "# RF.fit(X2_train_norm, y2_train_log)\n",
    "# pred = RF.predict(X2_test_norm)\n",
    "# Cenario 2 -> sem outliers\n",
    "# RF.fit(X2out_train_norm, y2out_train_log)\n",
    "# pred = RF.predict(X2out_test_norm)\n",
    "\n",
    "# Cenario 3 -> com outliers\n",
    "# RF.fit(X3_train_norm, y3_train_log)\n",
    "# pred = RF.predict(X3_test_norm)\n",
    "# Cenario 3 -> sem outliers\n",
    "# RF.fit(X3out_train_norm, y3out_train_log)\n",
    "# pred = RF.predict(X3out_test_norm)\n",
    "\n",
    "# Cenario 4 -> com outliers\n",
    "# RF.fit(X4_train_norm, y4_train_log)\n",
    "# pred = RF.predict(X4_test_norm)\n",
    "# Cenario 4 -> sem outliers\n",
    "RF.fit(X4out_train_norm, y4out_train_log)\n",
    "pred = RF.predict(X4out_test_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics for Random Forest\n",
    "\n",
    "# Reverter logaritmo dos valores preditos\n",
    "pred_orig_scale = np.exp(pred)\n",
    "\n",
    "# cenario 1 -> com outliers\n",
    "#reg_metrics = RegressionMetrics(np.exp(y1out_test_log), pred_orig_scale)\n",
    "# reg_metrics = RegressionMetrics(y1out_test, pred) # sem log\n",
    "\n",
    "# cenario 2 -> com outliers\n",
    "#reg_metrics = RegressionMetrics(np.exp(y2_test_log), pred_orig_scale)\n",
    "# cenario 2 -> sem outliers\n",
    "#reg_metrics = RegressionMetrics(np.exp(y2out_test_log), pred_orig_scale)\n",
    "\n",
    "# cenario 3 -> com outliers\n",
    "#reg_metrics = RegressionMetrics(np.exp(y3_test_log), pred_orig_scale)\n",
    "# cenario 3 -> sem outliers\n",
    "#reg_metrics = RegressionMetrics(np.exp(y3out_test_log), pred_orig_scale)\n",
    "\n",
    "# cenario 4 -> com outliers\n",
    "#reg_metrics = RegressionMetrics(np.exp(y4_test_log), pred_orig_scale)\n",
    "# cenario 4 -> sem outliers\n",
    "reg_metrics = RegressionMetrics(np.exp(y4out_test_log), pred_orig_scale)\n",
    "\n",
    "reg_metrics.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict with Adaboost Regressor\n",
    "AR = AdaBoostRegressor()\n",
    "\n",
    "# Cenario 1 -> com outliers \n",
    "# AR.fit(X1_train_norm, y1_train_log)\n",
    "# AR.fit(X1_train_norm, y1_train) # sem log\n",
    "# pred2 = AR.predict(X1_test_norm)\n",
    "# Cenario 1 -> sem outliers \n",
    "#AR.fit(X1out_train_norm, y1out_train_log)\n",
    "# AR.fit(X1out_train_norm, y1out_train) # sem log\n",
    "#pred2 = AR.predict(X1out_test_norm)\n",
    "\n",
    "# # Cenario 2 -> com outliers \n",
    "# AR.fit(X2_train_norm, y2_train_log)\n",
    "# pred2 = AR.predict(X2_test_norm)\n",
    "# Cenario 2 -> sem outliers \n",
    "# AR.fit(X2out_train_norm, y2out_train_log)\n",
    "# pred2 = AR.predict(X2out_test_norm)\n",
    "\n",
    "# Cenario 3 -> com outliers \n",
    "# AR.fit(X3_train_norm, y3_train_log)\n",
    "# pred2 = AR.predict(X3_test_norm)\n",
    "#Cenario 3 -> sem outliers \n",
    "# AR.fit(X3out_train_norm, y3out_train_log)\n",
    "# pred2 = AR.predict(X3out_test_norm)\n",
    "\n",
    "# Cenario 4 -> com outliers \n",
    "# AR.fit(X4_train_norm, y4_train_log)\n",
    "# pred2 = AR.predict(X4_test_norm)\n",
    "#Cenario 4 -> sem outliers \n",
    "AR.fit(X4out_train_norm, y4out_train_log)\n",
    "pred2 = AR.predict(X4out_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics for Adaboost\n",
    "\n",
    "# Reverter logaritmo dos valores preditos\n",
    "pred2_orig_scale = np.exp(pred2)\n",
    "\n",
    "# Cenario 1 -> com outliers \n",
    "# reg_metrics2 = RegressionMetrics(np.exp(y1_test_log), pred2_orig_scale)\n",
    "# reg_metrics2 = RegressionMetrics(y1_test, pred2)\n",
    "# Cenario 1 -> sem outliers \n",
    "#reg_metrics2 = RegressionMetrics(np.exp(y1out_test_log), pred2_orig_scale)\n",
    "# reg_metrics2 = RegressionMetrics(y1out_test, pred2)\n",
    "\n",
    "# # Cenario 2 -> com outliers \n",
    "# reg_metrics2 = RegressionMetrics(np.exp(y2_test_log), pred2_orig_scale)\n",
    "# Cenario 2 -> sem outliers \n",
    "#reg_metrics2 = RegressionMetrics(np.exp(y2out_test_log), pred2_orig_scale)\n",
    "\n",
    "# Cenario 3 -> com outliers \n",
    "#reg_metrics2 = RegressionMetrics(np.exp(y3_test_log), pred2_orig_scale)\n",
    "# Cenario 3 -> sem outliers \n",
    "#reg_metrics2 = RegressionMetrics(np.exp(y3out_test_log), pred2_orig_scale)\n",
    "\n",
    "# Cenario 4 -> com outliers \n",
    "#reg_metrics2 = RegressionMetrics(np.exp(y4_test_log), pred2_orig_scale)\n",
    "# Cenario 4 -> sem outliers \n",
    "reg_metrics2 = RegressionMetrics(np.exp(y4out_test_log), pred2_orig_scale)\n",
    "\n",
    "reg_metrics2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = tree.DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "\n",
    "# Cenario 1 -> com outliers \n",
    "# DT.fit(X1_train_norm, y1_train_log)\n",
    "# DT.fit(X1_train_norm, y1_train) # sem log\n",
    "# pred3 = DT.predict(X1_test_norm)\n",
    "# Cenario 1 -> sem outliers \n",
    "#DT.fit(X1out_train_norm, y1out_train_log)\n",
    "# DT.fit(X1out_train_norm, y1out_train) # sem log\n",
    "#pred3 = DT.predict(X1out_test_norm)\n",
    "\n",
    "# # Cenario 2 -> com outliers \n",
    "# DT.fit(X2_train_norm, y2_train_log)\n",
    "# pred3 = DT.predict(X2_test_norm)\n",
    "# Cenario 2 -> sem outliers \n",
    "# DT.fit(X2out_train_norm, y2out_train_log)\n",
    "# pred3 = DT.predict(X2out_test_norm)\n",
    "\n",
    "# Cenario 3 -> com outliers \n",
    "# DT.fit(X3_train_norm, y3_train_log)\n",
    "# pred3 = DT.predict(X3_test_norm)\n",
    "# Cenario 3 -> sem outliers \n",
    "# DT.fit(X3out_train_norm, y3out_train_log)\n",
    "# pred3 = DT.predict(X3out_test_norm)\n",
    "\n",
    "# Cenario 4 -> com outliers \n",
    "# DT.fit(X4_train_norm, y4_train_log)\n",
    "# pred3 = DT.predict(X4_test_norm)\n",
    "# Cenario 4 -> sem outliers \n",
    "DT.fit(X4out_train_norm, y4out_train_log)\n",
    "pred3 = DT.predict(X4out_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression metrics for Decision Tree\n",
    "# reg_metrics3 = RegressionMetrics(y1_test, pred3)\n",
    "# reg_metrics3 = RegressionMetrics(y1_test_log, pred3)\n",
    "\n",
    "# Reverter logaritmo dos valores preditos\n",
    "pred3_orig_scale = np.exp(pred3)\n",
    "\n",
    "# Cenario 1 -> com outliers \n",
    "# reg_metrics3 = RegressionMetrics(np.exp(y1_test_log), pred3_orig_scale)\n",
    "# reg_metrics3 = RegressionMetrics(y1_test, pred3)\n",
    "# Cenario 1 -> sem outliers \n",
    "#reg_metrics3 = RegressionMetrics(np.exp(y1out_test_log), pred3_orig_scale)\n",
    "# reg_metrics3 = RegressionMetrics(y1out_test, pred3)\n",
    "\n",
    "# # Cenario 2 -> com outliers \n",
    "# reg_metrics3 = RegressionMetrics(np.exp(y2_test_log), pred3_orig_scale)\n",
    "# Cenario 2 -> sem outliers \n",
    "#reg_metrics3 = RegressionMetrics(np.exp(y2out_test_log), pred3_orig_scale)\n",
    "\n",
    "# Cenario 3 -> com outliers \n",
    "#reg_metrics3 = RegressionMetrics(np.exp(y3_test_log), pred3_orig_scale)\n",
    "# Cenario 3 -> sem outliers \n",
    "#reg_metrics3 = RegressionMetrics(np.exp(y3out_test_log), pred3_orig_scale)\n",
    "\n",
    "# Cenario 4 -> com outliers \n",
    "#reg_metrics3 = RegressionMetrics(np.exp(y4_test_log), pred3_orig_scale)\n",
    "# Cenario 4 -> sem outliers \n",
    "reg_metrics3 = RegressionMetrics(np.exp(y4out_test_log), pred3_orig_scale)\n",
    "\n",
    "reg_metrics3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "tree.plot_tree(\n",
    "    # Cenario 2 -> com outliers \n",
    "    DT, feature_names=X2_train.columns, filled=True, rounded=True\n",
    ")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redes neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout + regularização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "# Parametros escolhidos pelo greedy\n",
    "modelo = Sequential()\n",
    "# Cenario 1 -> com outliers \n",
    "# modelo.add(Dense(units=32, activation='relu', input_dim=X1_train_norm.shape[1]))\n",
    "# Cenario 1 -> sem outliers \n",
    "#modelo.add(Dense(units=32, activation='relu', input_dim=X1out_train_norm.shape[1]))\n",
    "# # Cenario 2 -> com outliers \n",
    "# modelo.add(Dense(units=32, activation='relu', input_dim=X2_train_norm.shape[1]))\n",
    "# Cenario 2 -> sem outliers \n",
    "#modelo.add(Dense(units=32, activation='relu', input_dim=X2out_train_norm.shape[1]))\n",
    "# Cenario 3 -> com outliers \n",
    "#modelo.add(Dense(units=32, activation='relu', input_dim=X3_train_norm.shape[1]))\n",
    "# Cenario 3 -> sem outliers \n",
    "#modelo.add(Dense(units=32, activation='relu', input_dim=X3out_train_norm.shape[1]))\n",
    "# Cenario 4 -> com outliers \n",
    "#modelo.add(Dense(units=32, activation='relu', input_dim=X4_train_norm.shape[1]))\n",
    "# Cenario 4 -> sem outliers \n",
    "modelo.add(Dense(units=32, activation='relu', input_dim=X4out_train_norm.shape[1]))\n",
    "modelo.add(Dropout(0.3))  # Dropout para reduzir overfitting\n",
    "modelo.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "modelo.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Treinando a rede neural com mais épocas\n",
    "modelo.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "# Cenario 1 -> com outliers \n",
    "# resultado = modelo.fit(X1_train_norm, y1_train_log, epochs=500, batch_size=32, validation_data=(X1_test_norm, y1_test_log))\n",
    "# resultado = modelo.fit(X1_train_norm, y1_train, epochs=500, batch_size=32, validation_data=(X1_test_norm, y1_test))\n",
    "# pred4 = modelo.predict(X1_test_norm)\n",
    "# Cenario 1 -> sem outliers \n",
    "#resultado = modelo.fit(X1out_train_norm, y1out_train_log, epochs=500, batch_size=32, validation_data=(X1out_test_norm, y1out_test_log))\n",
    "# resultado = modelo.fit(X1out_train_norm, y1out_train, epochs=500, batch_size=32, validation_data=(X1out_test_norm, y1out_test))\n",
    "#pred4 = modelo.predict(X1out_test_norm)\n",
    "# # Cenario 2 -> com outliers \n",
    "# resultado = modelo.fit(X2_train_norm, y2_train_log, epochs=500, batch_size=32, validation_data=(X2_test_norm, y2_test_log))\n",
    "# pred4 = modelo.predict(X2_test_norm)\n",
    "# Cenario 2 -> sem outliers \n",
    "# resultado = modelo.fit(X2out_train_norm, y2out_train_log, epochs=500, batch_size=32, validation_data=(X2out_test_norm, y2out_test_log))\n",
    "# pred4 = modelo.predict(X2out_test_norm)\n",
    "# Cenario 3 -> com outliers \n",
    "# resultado = modelo.fit(X3_train_norm, y3_train_log, epochs=500, batch_size=32, validation_data=(X3_test_norm, y3_test_log))\n",
    "# pred4 = modelo.predict(X3_test_norm)\n",
    "# Cenario 3 -> sem outliers \n",
    "# resultado = modelo.fit(X3out_train_norm, y3out_train_log, epochs=500, batch_size=32, validation_data=(X3out_test_norm, y3out_test_log))\n",
    "# pred4 = modelo.predict(X3out_test_norm)\n",
    "# Cenario 4 -> com outliers \n",
    "# resultado = modelo.fit(X4_train_norm, y4_train_log, epochs=500, batch_size=32, validation_data=(X4_test_norm, y4_test_log))\n",
    "# pred4 = modelo.predict(X4_test_norm)\n",
    "# Cenario 4 -> sem outliers \n",
    "resultado = modelo.fit(X4out_train_norm, y4out_train_log, epochs=500, batch_size=32, validation_data=(X4out_test_norm, y4out_test_log))\n",
    "pred4 = modelo.predict(X4out_test_norm)\n",
    "\n",
    "\n",
    "#Plotando gráfico do histórico de treinamento\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(resultado.history['loss'])\n",
    "plt.plot(resultado.history['val_loss'])\n",
    "plt.title('Histórico de Treinamento')\n",
    "plt.ylabel('Função de custo')\n",
    "plt.xlabel('Épocas de treinamento')\n",
    "plt.legend(['Erro treino', 'Erro teste'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics redes neurais\n",
    "\n",
    "# Reverter logaritmo dos valores preditos\n",
    "pred4_orig_scale = np.exp(pred4)\n",
    "\n",
    "# Cenario 1 -> com outliers \n",
    "# reg_metrics4 = RegressionMetrics(np.exp(y1_test_log.ravel()), pred4_orig_scale.ravel())\n",
    "# reg_metrics4 = RegressionMetrics(y1_test.ravel(), pred4.ravel())\n",
    "# Cenario 1 -> sem outliers \n",
    "#reg_metrics4 = RegressionMetrics(np.exp(y1out_test_log.ravel()), pred4_orig_scale.ravel())\n",
    "# reg_metrics4 = RegressionMetrics(y1out_test.ravel(), pred4.ravel())\n",
    "\n",
    "# Cenario 2 -> com outliers \n",
    "#reg_metrics4 = RegressionMetrics(np.exp(y2_test_log.ravel()), pred4_orig_scale.ravel())\n",
    "# Cenario 2 -> sem outliers \n",
    "#reg_metrics4 = RegressionMetrics(np.exp(y2out_test_log.ravel()), pred4_orig_scale.ravel())\n",
    "\n",
    "# Cenario 3 -> com outliers \n",
    "#reg_metrics4 = RegressionMetrics(np.exp(y3_test_log.ravel()), pred4_orig_scale.ravel())\n",
    "# Cenario 3 -> sem outliers \n",
    "#reg_metrics4 = RegressionMetrics(np.exp(y3out_test_log.ravel()), pred4_orig_scale.ravel())\n",
    "\n",
    "# Cenario 4 -> com outliers \n",
    "#reg_metrics4 = RegressionMetrics(np.exp(y4_test_log.ravel()), pred4_orig_scale.ravel())\n",
    "# Cenario 4 -> sem outliers \n",
    "reg_metrics4 = RegressionMetrics(np.exp(y4out_test_log.ravel()), pred4_orig_scale.ravel())\n",
    "\n",
    "reg_metrics4.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### greedy search -> otimização de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Função para construir o modelo Keras\n",
    "def create_model(units1=64, units2=32, dropout_rate=0.5, kernel_regularizer=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=units1, activation='relu', input_dim=X1_train_norm.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=units2, activation='relu', kernel_regularizer=regularizers.l2(kernel_regularizer)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Definindo os parâmetros a serem otimizados\n",
    "param_grid = {\n",
    "    'units1': [32, 64, 128],\n",
    "    'units2': [16, 32, 64],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'kernel_regularizer': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Realizando a pesquisa em grade\n",
    "grid_search_results = []\n",
    "\n",
    "for units1 in param_grid['units1']:\n",
    "    for units2 in param_grid['units2']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "            for kernel_regularizer in param_grid['kernel_regularizer']:\n",
    "                model = create_model(units1=units1, units2=units2, dropout_rate=dropout_rate, kernel_regularizer=kernel_regularizer)\n",
    "                history = model.fit(X1_train_norm, y1_train_log, epochs=20, batch_size=32, verbose=0, validation_split=0.2, callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "                val_loss = history.history['val_loss'][-1]\n",
    "                grid_search_results.append({\n",
    "                    'units1': units1,\n",
    "                    'units2': units2,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'kernel_regularizer': kernel_regularizer,\n",
    "                    'val_loss': val_loss\n",
    "                })\n",
    "\n",
    "# Encontrando os melhores parâmetros\n",
    "best_params = min(grid_search_results, key=lambda x: x['val_loss'])\n",
    "\n",
    "# Imprimindo os resultados\n",
    "print(\"Melhores parâmetros encontrados: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparação dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressão deviation and accuracy values (for custom visualization)\n",
    "RG_dev = reg_metrics0.deviation_\n",
    "RG_acc = reg_metrics0.accuracy_\n",
    "\n",
    "# Random Forest deviation and accuracy values (for custom visualization)\n",
    "RF_dev = reg_metrics.deviation_\n",
    "RF_acc = reg_metrics.accuracy_\n",
    "# Adaboost deviation and accuracy values (for custom visualization)\n",
    "AR_dev = reg_metrics2.deviation_\n",
    "AR_acc = reg_metrics2.accuracy_\n",
    "# Decision Tree deviation and accuracy values (for custom visualization)\n",
    "DT_dev = reg_metrics3.deviation_\n",
    "DT_acc = reg_metrics3.accuracy_\n",
    "\n",
    "RN_dev=reg_metrics4.deviation_\n",
    "RN_acc=reg_metrics4.accuracy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REC curve - comparison of models\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5.5, 5))\n",
    "plt.plot(\n",
    "    RG_dev,\n",
    "    RG_acc,\n",
    "    color=\"purple\",\n",
    "    ls=\"--\",\n",
    "    lw=2,\n",
    "    label=\"Regression={}\".format(round(reg_metrics.auc_rec_, 3)),\n",
    ")\n",
    "plt.plot(\n",
    "    RF_dev,\n",
    "    RF_acc,\n",
    "    color=\"royalblue\",\n",
    "    ls=\"--\",\n",
    "    lw=2,\n",
    "    label=\"Random Forest={}\".format(round(reg_metrics.auc_rec_, 3)),\n",
    ")\n",
    "plt.plot(\n",
    "    AR_dev,\n",
    "    AR_acc,\n",
    "    color=\"darkorange\",\n",
    "    ls=\"-\",\n",
    "    lw=2,\n",
    "    label=\"AdaBoost={}\".format(round(reg_metrics2.auc_rec_, 3)),\n",
    ")\n",
    "plt.plot(\n",
    "    DT_dev,\n",
    "    DT_acc,\n",
    "    color=\"red\",\n",
    "    ls=\"-\",\n",
    "    lw=2,\n",
    "    label=\"DecisionTree={}\".format(round(reg_metrics3.auc_rec_, 3)),\n",
    ")\n",
    "plt.plot(\n",
    "    RN_dev,\n",
    "    RN_acc,\n",
    "    color=\"green\",\n",
    "    ls=\"-\",\n",
    "    lw=2,\n",
    "    label=\"NeuralNet={}\".format(round(reg_metrics4.auc_rec_, 3)),\n",
    ")\n",
    "plt.legend(loc=4)\n",
    "plt.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.xlabel(\"Deviation\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.concat(\n",
    "    [\n",
    "        reg_metrics0.get_metrics(),\n",
    "        reg_metrics.get_metrics(),\n",
    "        reg_metrics2.get_metrics(),\n",
    "        reg_metrics3.get_metrics(),\n",
    "        reg_metrics4.get_metrics(),\n",
    "    ]\n",
    ")\n",
    "metrics.index = [\"Regression\",\"RandomForest\", \"AdaBoost\", \"DecisionTree\",\"Neural Net\"]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample ( apenas nos dados de treino )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oversample cenário 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c/outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = subset_1['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "bins = [0, 10000, 30000, 60000, 100000, float('inf')]\n",
    "labels = ['Muito Baixo', 'Baixo', 'Médio', 'Alto', 'Luxo']\n",
    "y1 = pd.cut(y1, bins=bins, labels=labels)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "#Normalização\n",
    "features_num = X1.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "features_cat = X1.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    ")\n",
    "X1_train_norm = preprocessor.fit_transform(X1_train)\n",
    "X1_test_norm = preprocessor.transform(X1_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X1_train_smote, y1_train_smote = smote.fit_resample(X1_train_norm, y1_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7180685358255452\n",
      "Confusion Matrix:\n",
      " [[ 45   5   5   0  25]\n",
      " [  5 169   0  24  46]\n",
      " [  7   1  37   0   3]\n",
      " [  0  10   0  52   0]\n",
      " [ 22  22   4   2 158]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Alto       0.57      0.56      0.57        80\n",
      "       Baixo       0.82      0.69      0.75       244\n",
      "        Luxo       0.80      0.77      0.79        48\n",
      " Muito Baixo       0.67      0.84      0.74        62\n",
      "       Médio       0.68      0.76      0.72       208\n",
      "\n",
      "    accuracy                           0.72       642\n",
      "   macro avg       0.71      0.72      0.71       642\n",
      "weighted avg       0.73      0.72      0.72       642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, recall_score, \n",
    "                             precision_score, roc_auc_score, f1_score, log_loss, roc_curve)\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "svm_model.fit(X1_train_smote, y1_train_smote)\n",
    "\n",
    "# Previsões nos dados de teste\n",
    "y1_pred = svm_model.predict(X1_test_norm)\n",
    "\n",
    "# Avaliação do modelo\n",
    "print(\"Accuracy:\", accuracy_score(y1_test, y1_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y1_test, y1_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y1_test, y1_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# cenário 1\n",
    "# X1 = subset_1[['brand','model','model_year','milage','fuel_type','engine','transmission','ext_col','int_col','accident','clean_title']]\n",
    "# y1 = subset_1['price']\n",
    "X1out = subset_1_s_out[['brand','model','model_year','milage','fuel_type','engine','ext_col','int_col','accident','clean_title']] # tinha T2 » retirei...\n",
    "y1out = subset_1_s_out['price'] \n",
    "\n",
    "bins = [0, 10000, 30000, 60000, 100000, float('inf')]\n",
    "labels = ['Muito Baixo', 'Baixo', 'Médio', 'Alto', 'Luxo']\n",
    "\n",
    "# cenário 1\n",
    "# y1 = pd.cut(y1, bins=bins, labels=labels)\n",
    "y1out = pd.cut(y1out, bins=bins, labels=labels)\n",
    "\n",
    "# divisão em treino e teste\n",
    "# cenário 1\n",
    "# X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "X1out_train, X1out_test, y1out_train, y1out_test = train_test_split(X1out, y1out, test_size=0.2, random_state=42)# preços em categorias\n",
    "\n",
    "# Identificando tipos de dados\n",
    "# cenário 1\n",
    "# features_num = X1.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# features_cat = X1.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "features_num = X1out.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "features_cat = X1out.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Preprocessamento: escalonamento e codificação\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), features_num),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    ")\n",
    "\n",
    "# cenário 1\n",
    "# X1_train_norm = preprocessor.fit_transform(X1_train)\n",
    "# X1_test_norm = preprocessor.transform(X1_test)\n",
    "X1out_train_norm = preprocessor.fit_transform(X1out_train)\n",
    "X1out_test_norm = preprocessor.transform(X1out_test)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "# cenário 1\n",
    "# clf.fit(X1_train_norm, y1_train)\n",
    "clf.fit(X1out_train_norm, y1out_train)\n",
    "\n",
    "# cenário 1\n",
    "# y_pred = clf.predict(X1_test_norm)\n",
    "y_pred = clf.predict(X1out_test_norm)\n",
    "\n",
    "\n",
    "# cenário 1\n",
    "# print(classification_report(y1_test, y_pred))\n",
    "# cm=confusion_matrix(y1_test, y_pred)\n",
    "print(classification_report(y1out_test, y_pred))\n",
    "cm=confusion_matrix(y1out_test, y_pred)\n",
    "# matriz de confusão \n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Previsto')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(20,10)) # árvore\n",
    "# plot_tree(clf, feature_names=preprocessor.get_feature_names_out(), class_names=labels, filled=True, rounded=True, fontsize=12)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Probabilidades previstas para cada classe\n",
    "# cenário 1\n",
    "# y_score = clf.predict_proba(X1_test_norm)\n",
    "y_score = clf.predict_proba(X1out_test_norm)\n",
    "\n",
    "# Binarizando as classes\n",
    "# cenário 1\n",
    "# y_test_bin = label_binarize(y1_test, classes=labels)\n",
    "y_test_bin = label_binarize(y1out_test, classes=labels)\n",
    "\n",
    "# Calculando a curva ROC e a área sob a curva para cada classe\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(labels)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plotando a curva ROC para cada classe\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(labels)):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'ROC curve (area = {roc_auc[i]:.2f}) for {labels[i]}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Linha de não discriminação\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
